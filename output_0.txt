C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main.py
Files already downloaded and verified
Files already downloaded and verified
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Resnet                                   [1000, 32]                --
├─Sequential: 1-1                        [1000, 16, 32, 32]        --
│    └─Conv2d: 2-1                       [1000, 16, 32, 32]        432
│    └─BatchNorm2d: 2-2                  [1000, 16, 32, 32]        32
│    └─ReLU: 2-3                         [1000, 16, 32, 32]        --
├─Sequential: 1-2                        [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-4                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-1              [1000, 32, 16, 16]        13,952
│    │    └─Sequential: 3-2              [1000, 32, 16, 16]        576
│    │    └─ReLU: 3-3                    [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-5                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-4              [1000, 32, 16, 16]        18,560
│    │    └─Sequential: 3-5              [1000, 32, 16, 16]        --
│    │    └─ReLU: 3-6                    [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-6                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-7              [1000, 32, 16, 16]        18,560
│    │    └─Sequential: 3-8              [1000, 32, 16, 16]        --
│    │    └─ReLU: 3-9                    [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-7                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-10             [1000, 32, 16, 16]        18,560
│    │    └─Sequential: 3-11             [1000, 32, 16, 16]        --
│    │    └─ReLU: 3-12                   [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-8                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-13             [1000, 32, 16, 16]        18,560
│    │    └─Sequential: 3-14             [1000, 32, 16, 16]        --
│    │    └─ReLU: 3-15                   [1000, 32, 16, 16]        --
├─Sequential: 1-3                        [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-9                  [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-16             [1000, 64, 8, 8]          55,552
│    │    └─Sequential: 3-17             [1000, 64, 8, 8]          2,176
│    │    └─ReLU: 3-18                   [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-10                 [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-19             [1000, 64, 8, 8]          73,984
│    │    └─Sequential: 3-20             [1000, 64, 8, 8]          --
│    │    └─ReLU: 3-21                   [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-11                 [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-22             [1000, 64, 8, 8]          73,984
│    │    └─Sequential: 3-23             [1000, 64, 8, 8]          --
│    │    └─ReLU: 3-24                   [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-12                 [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-25             [1000, 64, 8, 8]          73,984
│    │    └─Sequential: 3-26             [1000, 64, 8, 8]          --
│    │    └─ReLU: 3-27                   [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-13                 [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-28             [1000, 64, 8, 8]          73,984
│    │    └─Sequential: 3-29             [1000, 64, 8, 8]          --
│    │    └─ReLU: 3-30                   [1000, 64, 8, 8]          --
├─Sequential: 1-4                        [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-14                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-31             [1000, 128, 4, 4]         221,696
│    │    └─Sequential: 3-32             [1000, 128, 4, 4]         8,448
│    │    └─ReLU: 3-33                   [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-15                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-34             [1000, 128, 4, 4]         295,424
│    │    └─Sequential: 3-35             [1000, 128, 4, 4]         --
│    │    └─ReLU: 3-36                   [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-16                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-37             [1000, 128, 4, 4]         295,424
│    │    └─Sequential: 3-38             [1000, 128, 4, 4]         --
│    │    └─ReLU: 3-39                   [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-17                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-40             [1000, 128, 4, 4]         295,424
│    │    └─Sequential: 3-41             [1000, 128, 4, 4]         --
│    │    └─ReLU: 3-42                   [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-18                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-43             [1000, 128, 4, 4]         295,424
│    │    └─Sequential: 3-44             [1000, 128, 4, 4]         --
│    │    └─ReLU: 3-45                   [1000, 128, 4, 4]         --
├─AdaptiveAvgPool2d: 1-5                 [1000, 128, 1, 1]         --
├─Sequential: 1-6                        [1000, 32]                --
│    └─Flatten: 2-19                     [1000, 128]               --
│    └─Linear: 2-20                      [1000, 32]                4,128
==========================================================================================
Total params: 1,858,864
Trainable params: 1,858,864
Non-trainable params: 0
Total mult-adds (G): 68.08
==========================================================================================
Input size (MB): 12.29
Forward/backward pass size (MB): 2785.54
Params size (MB): 7.44
Estimated Total Size (MB): 2805.26
==========================================================================================
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch : 0, Avg Loss : 4.4158
Epoch : 1, Avg Loss : 4.1891
Epoch : 2, Avg Loss : 4.1012
Epoch : 3, Avg Loss : 4.1130
Epoch : 4, Avg Loss : 4.1375
Epoch : 5, Avg Loss : 4.1553
Epoch : 6, Avg Loss : 4.0518
Epoch : 7, Avg Loss : 4.1921
Epoch : 8, Avg Loss : 4.6037
Epoch : 9, Avg Loss : 4.3693
Epoch : 10, Avg Loss : 4.2431
Epoch : 11, Avg Loss : 4.1277
Epoch : 12, Avg Loss : 4.0986
Epoch : 13, Avg Loss : 3.9932
Epoch : 14, Avg Loss : 4.1139
Epoch : 15, Avg Loss : 4.0502
Epoch : 16, Avg Loss : 4.0218
Epoch : 17, Avg Loss : 4.0359
Epoch : 18, Avg Loss : 3.9856
Epoch : 19, Avg Loss : 3.9845
Epoch : 20, Avg Loss : 3.9775
Epoch : 21, Avg Loss : 3.9641
Epoch : 22, Avg Loss : 3.9749
Epoch : 23, Avg Loss : 4.0138
Epoch : 24, Avg Loss : 3.9696
Epoch : 25, Avg Loss : 3.9704
Epoch : 26, Avg Loss : 3.9698
Epoch : 27, Avg Loss : 3.9999
Epoch : 28, Avg Loss : 3.9835
Epoch : 29, Avg Loss : 4.0202
FG 학습 완료. 이제 FG의 output을 실제 dataset의 label과 연결.
Epoch : 0, Avg Loss : 2.1503
Epoch : 1, Avg Loss : 1.9902
Epoch : 2, Avg Loss : 1.9656
Epoch : 3, Avg Loss : 1.9550
Epoch : 4, Avg Loss : 1.9489
Epoch : 5, Avg Loss : 1.9434
Epoch : 6, Avg Loss : 1.9392
Epoch : 7, Avg Loss : 1.9358
Epoch : 8, Avg Loss : 1.9324
Epoch : 9, Avg Loss : 1.9295
Epoch : 10, Avg Loss : 1.9276
Epoch : 11, Avg Loss : 1.9247
Epoch : 12, Avg Loss : 1.9234
Epoch : 13, Avg Loss : 1.9215
Epoch : 14, Avg Loss : 1.9200
Epoch : 15, Avg Loss : 1.9192
Epoch : 16, Avg Loss : 1.9179
Epoch : 17, Avg Loss : 1.9169
Epoch : 18, Avg Loss : 1.9163
Epoch : 19, Avg Loss : 1.9154
Epoch : 20, Avg Loss : 1.9144
Epoch : 21, Avg Loss : 1.9138
Epoch : 22, Avg Loss : 1.9133
Epoch : 23, Avg Loss : 1.9121
Epoch : 24, Avg Loss : 1.9119
Epoch : 25, Avg Loss : 1.9108
Epoch : 26, Avg Loss : 1.9106
Epoch : 27, Avg Loss : 1.9103
Epoch : 28, Avg Loss : 1.9098
Epoch : 29, Avg Loss : 1.9084
실제 test
총 개수 : 10000
 맞춘 개수 : 2871
 정확도: 28
 찍었을 때의 정확도 : 10

종료 코드 0(으)로 완료된 프로세스
