왜곡 이미지에도 이미지 정규화 적용

C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main.py
Files already downloaded and verified
Files already downloaded and verified
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
Sequential                                    [512, 32]                 --
├─Resnet: 1-1                                 [512, 32]                 --
│    └─Conv2d: 2-1                            [512, 64, 16, 16]         9,408
│    └─Sequential: 2-2                        [512, 64, 8, 8]           --
│    │    └─MaxPool2d: 3-1                    [512, 64, 8, 8]           --
│    │    └─ResnetBlock: 3-2                  [512, 64, 8, 8]           73,984
│    │    └─ResnetBlock: 3-3                  [512, 64, 8, 8]           73,984
│    └─Sequential: 2-3                        [512, 128, 8, 8]          --
│    │    └─ResnetBlock: 3-4                  [512, 128, 8, 8]          230,144
│    │    └─ResnetBlock: 3-5                  [512, 128, 8, 8]          295,424
│    │    └─ResnetBlock: 3-6                  [512, 128, 8, 8]          295,424
│    └─AdaptiveAvgPool2d: 2-4                 [512, 128, 1, 1]          --
│    └─Sequential: 2-5                        [512, 32]                 --
│    │    └─Flatten: 3-7                      [512, 128]                --
│    │    └─Linear: 3-8                       [512, 32]                 4,128
├─SimpleMLP: 1-2                              [512, 32]                 --
│    └─Linear: 2-6                            [512, 32]                 1,056
│    └─ReLU: 2-7                              [512, 32]                 --
===============================================================================================
Total params: 983,552
Trainable params: 983,552
Non-trainable params: 0
Total mult-adds (G): 32.91
===============================================================================================
Input size (MB): 6.29
Forward/backward pass size (MB): 671.35
Params size (MB): 3.93
Estimated Total Size (MB): 681.58
===============================================================================================
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch : 0, Avg Loss : 5.5334
Epoch : 1, Avg Loss : 5.4896
Epoch : 2, Avg Loss : 5.4696
Epoch : 3, Avg Loss : 5.4546
Epoch : 4, Avg Loss : 5.4309
Epoch : 5, Avg Loss : 5.4926
Epoch : 6, Avg Loss : 5.4870
Epoch : 7, Avg Loss : 5.4683
Epoch : 8, Avg Loss : 5.4819
Epoch : 9, Avg Loss : 5.4792
Epoch : 10, Avg Loss : 5.4892
Epoch : 11, Avg Loss : 5.4474
Epoch : 12, Avg Loss : 5.4737
Epoch : 13, Avg Loss : 5.4340
Epoch : 14, Avg Loss : 5.4743
Epoch : 15, Avg Loss : 5.4233
Epoch : 16, Avg Loss : 5.4552
Epoch : 17, Avg Loss : 5.4353
Epoch : 18, Avg Loss : 5.4325
Epoch : 19, Avg Loss : 5.4734
Epoch : 20, Avg Loss : 5.4792
Epoch : 21, Avg Loss : 5.4631
Epoch : 22, Avg Loss : 5.4570
Epoch : 23, Avg Loss : 5.4276
Epoch : 24, Avg Loss : 5.4517
Epoch : 25, Avg Loss : 5.4200
Epoch : 26, Avg Loss : 5.4093
Epoch : 27, Avg Loss : 5.4528
Epoch : 28, Avg Loss : 5.4511
Epoch : 29, Avg Loss : 5.4342
Epoch : 30, Avg Loss : 5.4071
Epoch : 31, Avg Loss : 5.4271
Epoch : 32, Avg Loss : 5.4335
Epoch : 33, Avg Loss : 5.3977
Epoch : 34, Avg Loss : 5.4423
Epoch : 35, Avg Loss : 5.4157
Epoch : 36, Avg Loss : 5.4075
Epoch : 37, Avg Loss : 5.4270
Epoch : 38, Avg Loss : 5.4019
Epoch : 39, Avg Loss : 5.4041
FG 학습 완료. 이제 FG의 output을 실제 dataset의 label과 연결.
Epoch : 0, Avg Loss : 1.9301
Epoch : 1, Avg Loss : 1.4978
Epoch : 2, Avg Loss : 1.4549
Epoch : 3, Avg Loss : 1.4436
Epoch : 4, Avg Loss : 1.4372
Epoch : 5, Avg Loss : 1.4335
Epoch : 6, Avg Loss : 1.4313
Epoch : 7, Avg Loss : 1.4291
Epoch : 8, Avg Loss : 1.4284
Epoch : 9, Avg Loss : 1.4276
Epoch : 10, Avg Loss : 1.4270
Epoch : 11, Avg Loss : 1.4259
Epoch : 12, Avg Loss : 1.4254
Epoch : 13, Avg Loss : 1.4249
Epoch : 14, Avg Loss : 1.4244
Epoch : 15, Avg Loss : 1.4239
Epoch : 16, Avg Loss : 1.4236
Epoch : 17, Avg Loss : 1.4223
Epoch : 18, Avg Loss : 1.4221
Epoch : 19, Avg Loss : 1.4212
Epoch : 20, Avg Loss : 1.4216
Epoch : 21, Avg Loss : 1.4210
Epoch : 22, Avg Loss : 1.4209
Epoch : 23, Avg Loss : 1.4207
Epoch : 24, Avg Loss : 1.4204
Epoch : 25, Avg Loss : 1.4204
Epoch : 26, Avg Loss : 1.4200
Epoch : 27, Avg Loss : 1.4193
Epoch : 28, Avg Loss : 1.4193
Epoch : 29, Avg Loss : 1.4187
Epoch : 30, Avg Loss : 1.4195
Epoch : 31, Avg Loss : 1.4190
Epoch : 32, Avg Loss : 1.4188
Epoch : 33, Avg Loss : 1.4184
Epoch : 34, Avg Loss : 1.4180
Epoch : 35, Avg Loss : 1.4184
Epoch : 36, Avg Loss : 1.4185
Epoch : 37, Avg Loss : 1.4181
Epoch : 38, Avg Loss : 1.4181
Epoch : 39, Avg Loss : 1.4175
실제 test
총 개수 : 10000
 맞춘 개수 : 4731
 정확도: 47
 찍었을 때의 정확도 : 10

종료 코드 0(으)로 완료된 프로세스
