C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main_with_originalres.py
Files already downloaded and verified
Files already downloaded and verified
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ResNet                                   [500, 32]                 --
├─Conv2d: 1-1                            [500, 64, 16, 16]         9,408
├─BatchNorm2d: 1-2                       [500, 64, 16, 16]         128
├─ReLU: 1-3                              [500, 64, 16, 16]         --
├─MaxPool2d: 1-4                         [500, 64, 8, 8]           --
├─Sequential: 1-5                        [500, 64, 8, 8]           --
│    └─BasicBlock: 2-1                   [500, 64, 8, 8]           --
│    │    └─Conv2d: 3-1                  [500, 64, 8, 8]           36,864
│    │    └─BatchNorm2d: 3-2             [500, 64, 8, 8]           128
│    │    └─ReLU: 3-3                    [500, 64, 8, 8]           --
│    │    └─Conv2d: 3-4                  [500, 64, 8, 8]           36,864
│    │    └─BatchNorm2d: 3-5             [500, 64, 8, 8]           128
│    │    └─ReLU: 3-6                    [500, 64, 8, 8]           --
│    └─BasicBlock: 2-2                   [500, 64, 8, 8]           --
│    │    └─Conv2d: 3-7                  [500, 64, 8, 8]           36,864
│    │    └─BatchNorm2d: 3-8             [500, 64, 8, 8]           128
│    │    └─ReLU: 3-9                    [500, 64, 8, 8]           --
│    │    └─Conv2d: 3-10                 [500, 64, 8, 8]           36,864
│    │    └─BatchNorm2d: 3-11            [500, 64, 8, 8]           128
│    │    └─ReLU: 3-12                   [500, 64, 8, 8]           --
├─Sequential: 1-6                        [500, 128, 4, 4]          --
│    └─BasicBlock: 2-3                   [500, 128, 4, 4]          --
│    │    └─Conv2d: 3-13                 [500, 128, 4, 4]          73,728
│    │    └─BatchNorm2d: 3-14            [500, 128, 4, 4]          256
│    │    └─ReLU: 3-15                   [500, 128, 4, 4]          --
│    │    └─Conv2d: 3-16                 [500, 128, 4, 4]          147,456
│    │    └─BatchNorm2d: 3-17            [500, 128, 4, 4]          256
│    │    └─Sequential: 3-18             [500, 128, 4, 4]          8,448
│    │    └─ReLU: 3-19                   [500, 128, 4, 4]          --
│    └─BasicBlock: 2-4                   [500, 128, 4, 4]          --
│    │    └─Conv2d: 3-20                 [500, 128, 4, 4]          147,456
│    │    └─BatchNorm2d: 3-21            [500, 128, 4, 4]          256
│    │    └─ReLU: 3-22                   [500, 128, 4, 4]          --
│    │    └─Conv2d: 3-23                 [500, 128, 4, 4]          147,456
│    │    └─BatchNorm2d: 3-24            [500, 128, 4, 4]          256
│    │    └─ReLU: 3-25                   [500, 128, 4, 4]          --
├─Sequential: 1-7                        [500, 256, 2, 2]          --
│    └─BasicBlock: 2-5                   [500, 256, 2, 2]          --
│    │    └─Conv2d: 3-26                 [500, 256, 2, 2]          294,912
│    │    └─BatchNorm2d: 3-27            [500, 256, 2, 2]          512
│    │    └─ReLU: 3-28                   [500, 256, 2, 2]          --
│    │    └─Conv2d: 3-29                 [500, 256, 2, 2]          589,824
│    │    └─BatchNorm2d: 3-30            [500, 256, 2, 2]          512
│    │    └─Sequential: 3-31             [500, 256, 2, 2]          33,280
│    │    └─ReLU: 3-32                   [500, 256, 2, 2]          --
│    └─BasicBlock: 2-6                   [500, 256, 2, 2]          --
│    │    └─Conv2d: 3-33                 [500, 256, 2, 2]          589,824
│    │    └─BatchNorm2d: 3-34            [500, 256, 2, 2]          512
│    │    └─ReLU: 3-35                   [500, 256, 2, 2]          --
│    │    └─Conv2d: 3-36                 [500, 256, 2, 2]          589,824
│    │    └─BatchNorm2d: 3-37            [500, 256, 2, 2]          512
│    │    └─ReLU: 3-38                   [500, 256, 2, 2]          --
├─Sequential: 1-8                        [500, 512, 1, 1]          --
│    └─BasicBlock: 2-7                   [500, 512, 1, 1]          --
│    │    └─Conv2d: 3-39                 [500, 512, 1, 1]          1,179,648
│    │    └─BatchNorm2d: 3-40            [500, 512, 1, 1]          1,024
│    │    └─ReLU: 3-41                   [500, 512, 1, 1]          --
│    │    └─Conv2d: 3-42                 [500, 512, 1, 1]          2,359,296
│    │    └─BatchNorm2d: 3-43            [500, 512, 1, 1]          1,024
│    │    └─Sequential: 3-44             [500, 512, 1, 1]          132,096
│    │    └─ReLU: 3-45                   [500, 512, 1, 1]          --
│    └─BasicBlock: 2-8                   [500, 512, 1, 1]          --
│    │    └─Conv2d: 3-46                 [500, 512, 1, 1]          2,359,296
│    │    └─BatchNorm2d: 3-47            [500, 512, 1, 1]          1,024
│    │    └─ReLU: 3-48                   [500, 512, 1, 1]          --
│    │    └─Conv2d: 3-49                 [500, 512, 1, 1]          2,359,296
│    │    └─BatchNorm2d: 3-50            [500, 512, 1, 1]          1,024
│    │    └─ReLU: 3-51                   [500, 512, 1, 1]          --
├─AdaptiveAvgPool2d: 1-9                 [500, 512, 1, 1]          --
├─Linear: 1-10                           [500, 32]                 16,416
==========================================================================================
Total params: 11,192,928
Trainable params: 11,192,928
Non-trainable params: 0
Total mult-adds (G): 18.52
==========================================================================================
Input size (MB): 6.14
Forward/backward pass size (MB): 405.63
Params size (MB): 44.77
Estimated Total Size (MB): 456.55
==========================================================================================
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch : 0, Avg Loss : 5.4296 lr: 0.00100000
Epoch : 1, Avg Loss : 4.9709 lr: 0.00095000
Epoch : 2, Avg Loss : 4.9100 lr: 0.00090250
Epoch : 3, Avg Loss : 4.8227 lr: 0.00085737
Epoch : 4, Avg Loss : 4.7083 lr: 0.00081451
Epoch : 5, Avg Loss : 4.6855 lr: 0.00077378
Epoch : 6, Avg Loss : 4.5973 lr: 0.00073509
Epoch : 7, Avg Loss : 4.6324 lr: 0.00069834
Epoch : 8, Avg Loss : 4.6291 lr: 0.00066342
Epoch : 9, Avg Loss : 4.5536 lr: 0.00063025
Epoch : 10, Avg Loss : 4.5202 lr: 0.00059874
Epoch : 11, Avg Loss : 4.4614 lr: 0.00056880
Epoch : 12, Avg Loss : 4.4884 lr: 0.00054036
Epoch : 13, Avg Loss : 4.4796 lr: 0.00051334
Epoch : 14, Avg Loss : 4.3816 lr: 0.00048767
Epoch : 15, Avg Loss : 4.4681 lr: 0.00046329
Epoch : 16, Avg Loss : 4.3066 lr: 0.00044013
Epoch : 17, Avg Loss : 4.3746 lr: 0.00041812
Epoch : 18, Avg Loss : 4.4167 lr: 0.00039721
Epoch : 19, Avg Loss : 4.3846 lr: 0.00037735
Epoch : 20, Avg Loss : 4.2453 lr: 0.00035849
Epoch : 21, Avg Loss : 4.0300 lr: 0.00034056
Epoch : 22, Avg Loss : 3.9155 lr: 0.00032353
Epoch : 23, Avg Loss : 3.8307 lr: 0.00030736
Epoch : 24, Avg Loss : 3.8340 lr: 0.00029199
Epoch : 25, Avg Loss : 3.7940 lr: 0.00027739
Epoch : 26, Avg Loss : 3.7645 lr: 0.00026352
Epoch : 27, Avg Loss : 3.7245 lr: 0.00025034
Epoch : 28, Avg Loss : 3.7550 lr: 0.00023783
Epoch : 29, Avg Loss : 3.6776 lr: 0.00022594
Epoch : 30, Avg Loss : 3.7027 lr: 0.00021464
Epoch : 31, Avg Loss : 3.6578 lr: 0.00020391
Epoch : 32, Avg Loss : 3.6676 lr: 0.00019371
Epoch : 33, Avg Loss : 3.7146 lr: 0.00018403
Epoch : 34, Avg Loss : 3.7177 lr: 0.00017482
Epoch : 35, Avg Loss : 3.6572 lr: 0.00016608
Epoch : 36, Avg Loss : 3.6518 lr: 0.00015778
Epoch : 37, Avg Loss : 3.6874 lr: 0.00014989
Epoch : 38, Avg Loss : 3.6126 lr: 0.00014240
Epoch : 39, Avg Loss : 3.6296 lr: 0.00013528
Epoch : 40, Avg Loss : 3.6056 lr: 0.00012851
Epoch : 41, Avg Loss : 3.5593 lr: 0.00012209
Epoch : 42, Avg Loss : 3.5916 lr: 0.00011598
Epoch : 43, Avg Loss : 3.6010 lr: 0.00011018
Epoch : 44, Avg Loss : 3.6161 lr: 0.00010467
Epoch : 45, Avg Loss : 3.5673 lr: 0.00009944
Epoch : 46, Avg Loss : 3.5870 lr: 0.00009447
Epoch : 47, Avg Loss : 3.5813 lr: 0.00008974
Epoch : 48, Avg Loss : 3.5877 lr: 0.00008526
Epoch : 49, Avg Loss : 3.5562 lr: 0.00008099
FG 학습 완료. 이제 F의 output을 실제 dataset의 label과 연결.
Epoch : 0, Avg Loss : 2.2689 lr: 0.00100000
Epoch : 1, Avg Loss : 2.0680 lr: 0.00095000
Epoch : 2, Avg Loss : 2.0188 lr: 0.00090250
Epoch : 3, Avg Loss : 1.9964 lr: 0.00085737
Epoch : 4, Avg Loss : 1.9812 lr: 0.00081451
Epoch : 5, Avg Loss : 1.9702 lr: 0.00077378
Epoch : 6, Avg Loss : 1.9599 lr: 0.00073509
Epoch : 7, Avg Loss : 1.9481 lr: 0.00069834
Epoch : 8, Avg Loss : 1.9378 lr: 0.00066342
Epoch : 9, Avg Loss : 1.9284 lr: 0.00063025
Epoch : 10, Avg Loss : 1.9196 lr: 0.00059874
Epoch : 11, Avg Loss : 1.9149 lr: 0.00056880
Epoch : 12, Avg Loss : 1.9089 lr: 0.00054036
Epoch : 13, Avg Loss : 1.9034 lr: 0.00051334
Epoch : 14, Avg Loss : 1.9000 lr: 0.00048767
Epoch : 15, Avg Loss : 1.8954 lr: 0.00046329
Epoch : 16, Avg Loss : 1.8916 lr: 0.00044013
Epoch : 17, Avg Loss : 1.8884 lr: 0.00041812
Epoch : 18, Avg Loss : 1.8860 lr: 0.00039721
Epoch : 19, Avg Loss : 1.8833 lr: 0.00037735
Epoch : 20, Avg Loss : 1.8817 lr: 0.00035849
Epoch : 21, Avg Loss : 1.8780 lr: 0.00034056
Epoch : 22, Avg Loss : 1.8765 lr: 0.00032353
Epoch : 23, Avg Loss : 1.8748 lr: 0.00030736
Epoch : 24, Avg Loss : 1.8730 lr: 0.00029199
Epoch : 25, Avg Loss : 1.8721 lr: 0.00027739
Epoch : 26, Avg Loss : 1.8722 lr: 0.00026352
Epoch : 27, Avg Loss : 1.8696 lr: 0.00025034
Epoch : 28, Avg Loss : 1.8694 lr: 0.00023783
Epoch : 29, Avg Loss : 1.8679 lr: 0.00022594
Epoch : 30, Avg Loss : 1.8671 lr: 0.00021464
Epoch : 31, Avg Loss : 1.8663 lr: 0.00020391
Epoch : 32, Avg Loss : 1.8643 lr: 0.00019371
Epoch : 33, Avg Loss : 1.8639 lr: 0.00018403
Epoch : 34, Avg Loss : 1.8637 lr: 0.00017482
Epoch : 35, Avg Loss : 1.8626 lr: 0.00016608
Epoch : 36, Avg Loss : 1.8616 lr: 0.00015778
Epoch : 37, Avg Loss : 1.8608 lr: 0.00014989
Epoch : 38, Avg Loss : 1.8589 lr: 0.00014240
Epoch : 39, Avg Loss : 1.8601 lr: 0.00013528
Epoch : 40, Avg Loss : 1.8604 lr: 0.00012851
Epoch : 41, Avg Loss : 1.8595 lr: 0.00012209
Epoch : 42, Avg Loss : 1.8587 lr: 0.00011598
Epoch : 43, Avg Loss : 1.8579 lr: 0.00011018
Epoch : 44, Avg Loss : 1.8583 lr: 0.00010467
Epoch : 45, Avg Loss : 1.8583 lr: 0.00009944
Epoch : 46, Avg Loss : 1.8575 lr: 0.00009447
Epoch : 47, Avg Loss : 1.8573 lr: 0.00008974
Epoch : 48, Avg Loss : 1.8581 lr: 0.00008526
Epoch : 49, Avg Loss : 1.8570 lr: 0.00008099
실제 test
총 개수 : 10000
 맞춘 개수 : 3147
 정확도: 31
 찍었을 때의 정확도 : 10