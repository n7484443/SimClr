C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main_sam.py
Files already downloaded and verified
Files already downloaded and verified
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
SimCLR                                        [64, 512]                 --
├─ResNet: 1-1                                 [64, 512]                 --
│    └─Conv2d: 2-1                            [64, 64, 32, 32]          1,728
│    └─BatchNorm2d: 2-2                       [64, 64, 32, 32]          128
│    └─ReLU: 2-3                              [64, 64, 32, 32]          --
│    └─Identity: 2-4                          [64, 64, 32, 32]          --
│    └─Sequential: 2-5                        [64, 64, 32, 32]          --
│    │    └─BasicBlock: 3-1                   [64, 64, 32, 32]          73,984
│    │    └─BasicBlock: 3-2                   [64, 64, 32, 32]          73,984
│    └─Sequential: 2-6                        [64, 128, 16, 16]         --
│    │    └─BasicBlock: 3-3                   [64, 128, 16, 16]         230,144
│    │    └─BasicBlock: 3-4                   [64, 128, 16, 16]         295,424
│    └─Sequential: 2-7                        [64, 256, 8, 8]           --
│    │    └─BasicBlock: 3-5                   [64, 256, 8, 8]           919,040
│    │    └─BasicBlock: 3-6                   [64, 256, 8, 8]           1,180,672
│    └─Sequential: 2-8                        [64, 512, 4, 4]           --
│    │    └─BasicBlock: 3-7                   [64, 512, 4, 4]           3,673,088
│    │    └─BasicBlock: 3-8                   [64, 512, 4, 4]           4,720,640
│    └─AdaptiveAvgPool2d: 2-9                 [64, 512, 1, 1]           --
│    └─Identity: 2-10                         [64, 512]                 --
├─Sequential: 1-2                             [64, 128]                 --
│    └─Linear: 2-11                           [64, 512]                 262,144
│    └─ReLU: 2-12                             [64, 512]                 --
│    └─Linear: 2-13                           [64, 128]                 65,536
===============================================================================================
Total params: 11,496,512
Trainable params: 11,496,512
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 35.57
===============================================================================================
Input size (MB): 0.79
Forward/backward pass size (MB): 629.47
Params size (MB): 45.99
Estimated Total Size (MB): 676.25
===============================================================================================
Epoch 1: 100%|██████████| 782/782 [02:29<00:00,  5.23batch/s]
Avg Loss : 4.5716 Learning Late: 0.6000
Epoch 2: 100%|██████████| 782/782 [02:28<00:00,  5.25batch/s]
Avg Loss : 4.3874 Learning Late: 0.6000
Epoch 3: 100%|██████████| 782/782 [02:29<00:00,  5.24batch/s]
Avg Loss : 4.3280 Learning Late: 0.6000
Epoch 4: 100%|██████████| 782/782 [02:28<00:00,  5.28batch/s]
Avg Loss : 4.2580 Learning Late: 0.6000
Epoch 5: 100%|██████████| 782/782 [02:29<00:00,  5.23batch/s]
Avg Loss : 4.2079 Learning Late: 0.6000
Epoch 6: 100%|██████████| 782/782 [02:29<00:00,  5.24batch/s]
Avg Loss : 4.0390 Learning Late: 0.6000
Epoch 7: 100%|██████████| 782/782 [02:28<00:00,  5.25batch/s]
Avg Loss : 3.8655 Learning Late: 0.6000
Epoch 8: 100%|██████████| 782/782 [02:28<00:00,  5.25batch/s]
Avg Loss : 3.7105 Learning Late: 0.6000
Epoch 9: 100%|██████████| 782/782 [02:28<00:00,  5.28batch/s]
Avg Loss : 3.5096 Learning Late: 0.6000
Epoch 10: 100%|██████████| 782/782 [02:29<00:00,  5.24batch/s]
Avg Loss : 3.3061 Learning Late: 0.6000
Epoch 11: 100%|██████████| 782/782 [02:27<00:00,  5.32batch/s]
Avg Loss : 3.0070 Learning Late: 0.5998
Epoch 12: 100%|██████████| 782/782 [02:27<00:00,  5.31batch/s]
Avg Loss : 2.6919 Learning Late: 0.5993
Epoch 13: 100%|██████████| 782/782 [02:27<00:00,  5.32batch/s]
Avg Loss : 2.5299 Learning Late: 0.5984
Epoch 14: 100%|██████████| 782/782 [02:28<00:00,  5.27batch/s]
Avg Loss : 2.2730 Learning Late: 0.5971
Epoch 15: 100%|██████████| 782/782 [02:28<00:00,  5.25batch/s]
Avg Loss : 1.8514 Learning Late: 0.5954
Epoch 16: 100%|██████████| 782/782 [02:28<00:00,  5.27batch/s]
Avg Loss : 1.5705 Learning Late: 0.5934
Epoch 17: 100%|██████████| 782/782 [02:27<00:00,  5.31batch/s]
Avg Loss : 1.2697 Learning Late: 0.5911
Epoch 18: 100%|██████████| 782/782 [02:27<00:00,  5.30batch/s]
Avg Loss : 1.0534 Learning Late: 0.5884
Epoch 19: 100%|██████████| 782/782 [02:28<00:00,  5.26batch/s]
Avg Loss : 0.9114 Learning Late: 0.5853
Epoch 20: 100%|██████████| 782/782 [02:29<00:00,  5.24batch/s]
Avg Loss : 0.7761 Learning Late: 0.5819
Epoch 21: 100%|██████████| 782/782 [02:27<00:00,  5.30batch/s]
Avg Loss : 0.6923 Learning Late: 0.5782
Epoch 22: 100%|██████████| 782/782 [02:27<00:00,  5.30batch/s]
Avg Loss : 0.6387 Learning Late: 0.5741
Epoch 23: 100%|██████████| 782/782 [02:27<00:00,  5.30batch/s]
Avg Loss : 0.5810 Learning Late: 0.5696
Epoch 24: 100%|██████████| 782/782 [02:28<00:00,  5.28batch/s]
Avg Loss : 0.5225 Learning Late: 0.5649
Epoch 25: 100%|██████████| 782/782 [02:27<00:00,  5.31batch/s]
Avg Loss : 0.5055 Learning Late: 0.5598
Epoch 26: 100%|██████████| 782/782 [02:27<00:00,  5.31batch/s]
Avg Loss : 0.4483 Learning Late: 0.5544
Epoch 27: 100%|██████████| 782/782 [02:27<00:00,  5.31batch/s]
Avg Loss : 0.4245 Learning Late: 0.5487
Epoch 28: 100%|██████████| 782/782 [02:29<00:00,  5.22batch/s]
Avg Loss : 0.4087 Learning Late: 0.5427
Epoch 29: 100%|██████████| 782/782 [02:27<00:00,  5.29batch/s]
Avg Loss : 0.3918 Learning Late: 0.5364
Epoch 30: 100%|██████████| 782/782 [02:29<00:00,  5.24batch/s]
Avg Loss : 0.3846 Learning Late: 0.5298
Epoch 31: 100%|██████████| 782/782 [02:28<00:00,  5.25batch/s]
Avg Loss : 0.3598 Learning Late: 0.5229
Epoch 32: 100%|██████████| 782/782 [02:27<00:00,  5.29batch/s]
Avg Loss : 0.3384 Learning Late: 0.5158
Epoch 33: 100%|██████████| 782/782 [02:27<00:00,  5.30batch/s]
Avg Loss : 0.3179 Learning Late: 0.5084
Epoch 34: 100%|██████████| 782/782 [02:27<00:00,  5.31batch/s]
Avg Loss : 0.3023 Learning Late: 0.5007
Epoch 35: 100%|██████████| 782/782 [02:27<00:00,  5.31batch/s]
Avg Loss : 0.2887 Learning Late: 0.4928
Epoch 36: 100%|██████████| 782/782 [02:27<00:00,  5.32batch/s]
Avg Loss : 0.2881 Learning Late: 0.4847
Epoch 37: 100%|██████████| 782/782 [02:29<00:00,  5.23batch/s]
Avg Loss : 0.2741 Learning Late: 0.4763
Epoch 38: 100%|██████████| 782/782 [02:29<00:00,  5.22batch/s]
Avg Loss : 0.2712 Learning Late: 0.4678
Epoch 39: 100%|██████████| 782/782 [02:29<00:00,  5.24batch/s]
Avg Loss : 0.2864 Learning Late: 0.4590
Epoch 40: 100%|██████████| 782/782 [02:29<00:00,  5.23batch/s]
Avg Loss : 0.2442 Learning Late: 0.4500
Epoch 41: 100%|██████████| 782/782 [02:27<00:00,  5.30batch/s]
Avg Loss : 0.2278 Learning Late: 0.4408
Epoch 42: 100%|██████████| 782/782 [02:29<00:00,  5.24batch/s]
Avg Loss : 0.2251 Learning Late: 0.4315
Epoch 43: 100%|██████████| 782/782 [02:28<00:00,  5.28batch/s]
Avg Loss : 0.2389 Learning Late: 0.4220
Epoch 44: 100%|██████████| 782/782 [02:28<00:00,  5.28batch/s]
Avg Loss : 0.2192 Learning Late: 0.4124
Epoch 45: 100%|██████████| 782/782 [02:27<00:00,  5.31batch/s]
Avg Loss : 0.2031 Learning Late: 0.4026
Epoch 46: 100%|██████████| 782/782 [02:28<00:00,  5.25batch/s]
Avg Loss : 0.2042 Learning Late: 0.3927
Epoch 47: 100%|██████████| 782/782 [02:27<00:00,  5.31batch/s]
Avg Loss : 0.2002 Learning Late: 0.3827
Epoch 48: 100%|██████████| 782/782 [02:27<00:00,  5.32batch/s]
Avg Loss : 0.1927 Learning Late: 0.3726
Epoch 49: 100%|██████████| 782/782 [02:27<00:00,  5.32batch/s]
Avg Loss : 0.1989 Learning Late: 0.3624
Epoch 50: 100%|██████████| 782/782 [02:30<00:00,  5.18batch/s]
Avg Loss : 0.1749 Learning Late: 0.3521
Epoch 51: 100%|██████████| 782/782 [02:36<00:00,  5.00batch/s]
Avg Loss : 0.1860 Learning Late: 0.3418
Epoch 52: 100%|██████████| 782/782 [02:38<00:00,  4.92batch/s]
Avg Loss : 0.1708 Learning Late: 0.3314
Epoch 53: 100%|██████████| 782/782 [02:35<00:00,  5.02batch/s]
Avg Loss : 0.1754 Learning Late: 0.3209
Epoch 54: 100%|██████████| 782/782 [02:35<00:00,  5.02batch/s]
Avg Loss : 0.1765 Learning Late: 0.3105
Epoch 55: 100%|██████████| 782/782 [02:30<00:00,  5.20batch/s]
Avg Loss : 0.1652 Learning Late: 0.3000
Epoch 56: 100%|██████████| 782/782 [02:29<00:00,  5.23batch/s]
Avg Loss : 0.1608 Learning Late: 0.2895
Epoch 57: 100%|██████████| 782/782 [02:29<00:00,  5.23batch/s]
Avg Loss : 0.1451 Learning Late: 0.2791
Epoch 58: 100%|██████████| 782/782 [02:28<00:00,  5.28batch/s]
Avg Loss : 0.1603 Learning Late: 0.2686
Epoch 59: 100%|██████████| 782/782 [02:27<00:00,  5.29batch/s]
Avg Loss : 0.1517 Learning Late: 0.2582
Epoch 60: 100%|██████████| 782/782 [02:28<00:00,  5.27batch/s]
Avg Loss : 0.1506 Learning Late: 0.2479
Epoch 61: 100%|██████████| 782/782 [02:27<00:00,  5.29batch/s]
Avg Loss : 0.1504 Learning Late: 0.2376
Epoch 62: 100%|██████████| 782/782 [02:29<00:00,  5.23batch/s]
Avg Loss : 0.1429 Learning Late: 0.2274
Epoch 63: 100%|██████████| 782/782 [02:30<00:00,  5.21batch/s]
Avg Loss : 0.1316 Learning Late: 0.2173
Epoch 64: 100%|██████████| 782/782 [02:29<00:00,  5.21batch/s]
Avg Loss : 0.1428 Learning Late: 0.2073
Epoch 65: 100%|██████████| 782/782 [02:30<00:00,  5.21batch/s]
Avg Loss : 0.1436 Learning Late: 0.1974
Epoch 66: 100%|██████████| 782/782 [02:30<00:00,  5.21batch/s]
Avg Loss : 0.1307 Learning Late: 0.1876
Epoch 67: 100%|██████████| 782/782 [02:30<00:00,  5.20batch/s]
Avg Loss : 0.1319 Learning Late: 0.1780
Epoch 68: 100%|██████████| 782/782 [02:28<00:00,  5.27batch/s]
Avg Loss : 0.1361 Learning Late: 0.1685
Epoch 69: 100%|██████████| 782/782 [02:30<00:00,  5.21batch/s]
Avg Loss : 0.1225 Learning Late: 0.1592
Epoch 70: 100%|██████████| 782/782 [02:30<00:00,  5.20batch/s]
Avg Loss : 0.1244 Learning Late: 0.1500
Epoch 71: 100%|██████████| 782/782 [02:30<00:00,  5.21batch/s]
Avg Loss : 0.1163 Learning Late: 0.1410
Epoch 72: 100%|██████████| 782/782 [02:30<00:00,  5.21batch/s]
Avg Loss : 0.1229 Learning Late: 0.1322
Epoch 73: 100%|██████████| 782/782 [02:30<00:00,  5.21batch/s]
Avg Loss : 0.1163 Learning Late: 0.1237
Epoch 74: 100%|██████████| 782/782 [02:30<00:00,  5.21batch/s]
Avg Loss : 0.1124 Learning Late: 0.1153
Epoch 75: 100%|██████████| 782/782 [02:30<00:00,  5.20batch/s]
Avg Loss : 0.1260 Learning Late: 0.1072
Epoch 76: 100%|██████████| 782/782 [02:32<00:00,  5.11batch/s]
Avg Loss : 0.1145 Learning Late: 0.0993
Epoch 77: 100%|██████████| 782/782 [02:28<00:00,  5.27batch/s]
Avg Loss : 0.1160 Learning Late: 0.0916
Epoch 78: 100%|██████████| 782/782 [02:28<00:00,  5.27batch/s]
Avg Loss : 0.1127 Learning Late: 0.0842
Epoch 79: 100%|██████████| 782/782 [02:28<00:00,  5.28batch/s]
Avg Loss : 0.1119 Learning Late: 0.0771
Epoch 80: 100%|██████████| 782/782 [02:31<00:00,  5.16batch/s]
Avg Loss : 0.1143 Learning Late: 0.0702
Epoch 81: 100%|██████████| 782/782 [02:31<00:00,  5.17batch/s]
Avg Loss : 0.1140 Learning Late: 0.0636
Epoch 82: 100%|██████████| 782/782 [02:30<00:00,  5.21batch/s]
Avg Loss : 0.1064 Learning Late: 0.0573
Epoch 83: 100%|██████████| 782/782 [02:30<00:00,  5.21batch/s]
Avg Loss : 0.1015 Learning Late: 0.0513
Epoch 84: 100%|██████████| 782/782 [02:29<00:00,  5.21batch/s]
Avg Loss : 0.0988 Learning Late: 0.0456
Epoch 85: 100%|██████████| 782/782 [02:29<00:00,  5.22batch/s]
Avg Loss : 0.0977 Learning Late: 0.0402
Epoch 86: 100%|██████████| 782/782 [02:31<00:00,  5.17batch/s]
Avg Loss : 0.1023 Learning Late: 0.0351
Epoch 87: 100%|██████████| 782/782 [02:30<00:00,  5.19batch/s]
Avg Loss : 0.1009 Learning Late: 0.0304
Epoch 88: 100%|██████████| 782/782 [02:27<00:00,  5.29batch/s]
Avg Loss : 0.0981 Learning Late: 0.0259
Epoch 89: 100%|██████████| 782/782 [02:27<00:00,  5.30batch/s]
Avg Loss : 0.1028 Learning Late: 0.0218
Epoch 90: 100%|██████████| 782/782 [02:27<00:00,  5.29batch/s]
Avg Loss : 0.0967 Learning Late: 0.0181
Epoch 91: 100%|██████████| 782/782 [02:27<00:00,  5.29batch/s]
Avg Loss : 0.0973 Learning Late: 0.0147
Epoch 92: 100%|██████████| 782/782 [02:29<00:00,  5.23batch/s]
Avg Loss : 0.1034 Learning Late: 0.0116
Epoch 93: 100%|██████████| 782/782 [02:30<00:00,  5.20batch/s]
Avg Loss : 0.0988 Learning Late: 0.0089
Epoch 94: 100%|██████████| 782/782 [02:30<00:00,  5.20batch/s]
Avg Loss : 0.1003 Learning Late: 0.0066
Epoch 95: 100%|██████████| 782/782 [02:30<00:00,  5.21batch/s]
Avg Loss : 0.0943 Learning Late: 0.0046
Epoch 96: 100%|██████████| 782/782 [02:30<00:00,  5.19batch/s]
Avg Loss : 0.1048 Learning Late: 0.0029
Epoch 97: 100%|██████████| 782/782 [02:29<00:00,  5.22batch/s]
Avg Loss : 0.0952 Learning Late: 0.0016
Epoch 98: 100%|██████████| 782/782 [02:30<00:00,  5.21batch/s]
Avg Loss : 0.0974 Learning Late: 0.0007
Epoch 99: 100%|██████████| 782/782 [02:29<00:00,  5.24batch/s]
Avg Loss : 0.0981 Learning Late: 0.0002
Epoch 100: 100%|██████████| 782/782 [02:29<00:00,  5.23batch/s]
Avg Loss : 0.1003 Learning Late: 0.0000
Epoch 1: 100%|██████████| 782/782 [00:44<00:00, 17.50batch/s]
Avg Loss : 3.3508 Validation Loss : 3.0203 Learning Late: 0.0010 Accuracy: 47.3700
Epoch 2: 100%|██████████| 782/782 [00:44<00:00, 17.58batch/s]
Avg Loss : 2.1272 Validation Loss : 1.8783 Learning Late: 0.0010 Accuracy: 57.5300
Epoch 3: 100%|██████████| 782/782 [00:44<00:00, 17.74batch/s]
Avg Loss : 1.9289 Validation Loss : 1.8692 Learning Late: 0.0010 Accuracy: 57.1000
Epoch 4: 100%|██████████| 782/782 [00:43<00:00, 17.77batch/s]
Avg Loss : 1.9016 Validation Loss : 1.9479 Learning Late: 0.0010 Accuracy: 56.9000
Epoch 5: 100%|██████████| 782/782 [00:43<00:00, 17.77batch/s]
Avg Loss : 1.8865 Validation Loss : 1.9368 Learning Late: 0.0010 Accuracy: 56.5300
Epoch 6: 100%|██████████| 782/782 [00:44<00:00, 17.42batch/s]
Avg Loss : 1.8751 Validation Loss : 2.0359 Learning Late: 0.0010 Accuracy: 56.6900
Epoch 7: 100%|██████████| 782/782 [00:45<00:00, 17.25batch/s]
Avg Loss : 1.8978 Validation Loss : 2.0352 Learning Late: 0.0010 Accuracy: 57.2200
Epoch 8: 100%|██████████| 782/782 [00:44<00:00, 17.52batch/s]
Avg Loss : 1.8886 Validation Loss : 2.4738 Learning Late: 0.0010 Accuracy: 51.4900
Epoch 9: 100%|██████████| 782/782 [00:44<00:00, 17.53batch/s]
Avg Loss : 1.8799 Validation Loss : 2.1244 Learning Late: 0.0010 Accuracy: 54.5000
Epoch 10: 100%|██████████| 782/782 [00:44<00:00, 17.43batch/s]
Avg Loss : 1.8875 Validation Loss : 1.6031 Learning Late: 0.0010 Accuracy: 60.7000
Epoch 11: 100%|██████████| 782/782 [00:44<00:00, 17.55batch/s]
Avg Loss : 1.8383 Validation Loss : 2.1235 Learning Late: 0.0010 Accuracy: 54.7000
Epoch 12: 100%|██████████| 782/782 [00:44<00:00, 17.69batch/s]
Avg Loss : 1.8737 Validation Loss : 2.0474 Learning Late: 0.0010 Accuracy: 56.4700
Epoch 13: 100%|██████████| 782/782 [00:44<00:00, 17.57batch/s]
Avg Loss : 1.8259 Validation Loss : 1.7665 Learning Late: 0.0010 Accuracy: 58.8300
Epoch 14: 100%|██████████| 782/782 [00:44<00:00, 17.61batch/s]
Avg Loss : 1.8652 Validation Loss : 1.8581 Learning Late: 0.0010 Accuracy: 58.9700
Epoch 15: 100%|██████████| 782/782 [00:44<00:00, 17.69batch/s]
Avg Loss : 1.8580 Validation Loss : 2.0074 Learning Late: 0.0010 Accuracy: 58.0900
Epoch 16: 100%|██████████| 782/782 [00:44<00:00, 17.62batch/s]
Avg Loss : 1.8331 Validation Loss : 1.8830 Learning Late: 0.0010 Accuracy: 58.0600
Epoch 17: 100%|██████████| 782/782 [00:44<00:00, 17.63batch/s]
Avg Loss : 1.8728 Validation Loss : 1.8065 Learning Late: 0.0010 Accuracy: 58.9800
Epoch 18: 100%|██████████| 782/782 [00:44<00:00, 17.65batch/s]
Avg Loss : 1.8220 Validation Loss : 1.6291 Learning Late: 0.0010 Accuracy: 60.4900
Epoch 19: 100%|██████████| 782/782 [00:44<00:00, 17.67batch/s]
Avg Loss : 1.7921 Validation Loss : 1.9032 Learning Late: 0.0010 Accuracy: 56.0500
Epoch 20: 100%|██████████| 782/782 [00:44<00:00, 17.66batch/s]
Avg Loss : 1.8087 Validation Loss : 2.1162 Learning Late: 0.0010 Accuracy: 54.9900
Epoch 21: 100%|██████████| 782/782 [00:44<00:00, 17.58batch/s]
Avg Loss : 1.8343 Validation Loss : 2.2581 Learning Late: 0.0010 Accuracy: 55.1600
Epoch 22: 100%|██████████| 782/782 [00:44<00:00, 17.60batch/s]
Avg Loss : 1.8307 Validation Loss : 1.9173 Learning Late: 0.0010 Accuracy: 56.6900
Epoch 23: 100%|██████████| 782/782 [00:44<00:00, 17.64batch/s]
Avg Loss : 1.7880 Validation Loss : 2.1225 Learning Late: 0.0009 Accuracy: 55.6800
Epoch 24: 100%|██████████| 782/782 [00:44<00:00, 17.65batch/s]
Avg Loss : 1.7588 Validation Loss : 2.0940 Learning Late: 0.0009 Accuracy: 55.4600
Epoch 25: 100%|██████████| 782/782 [00:44<00:00, 17.58batch/s]
Avg Loss : 1.7700 Validation Loss : 1.7970 Learning Late: 0.0009 Accuracy: 57.4900
Epoch 26: 100%|██████████| 782/782 [00:44<00:00, 17.57batch/s]
Avg Loss : 1.7919 Validation Loss : 1.7967 Learning Late: 0.0009 Accuracy: 57.0800
Epoch 27: 100%|██████████| 782/782 [00:44<00:00, 17.65batch/s]
Avg Loss : 1.7445 Validation Loss : 1.6587 Learning Late: 0.0009 Accuracy: 58.2600
Epoch 28: 100%|██████████| 782/782 [00:44<00:00, 17.65batch/s]
Avg Loss : 1.7203 Validation Loss : 1.9863 Learning Late: 0.0009 Accuracy: 55.4500
Epoch 29: 100%|██████████| 782/782 [00:44<00:00, 17.56batch/s]
Avg Loss : 1.7580 Validation Loss : 1.8392 Learning Late: 0.0009 Accuracy: 57.4900
Epoch 30: 100%|██████████| 782/782 [00:44<00:00, 17.59batch/s]
Avg Loss : 1.7319 Validation Loss : 1.5762 Learning Late: 0.0009 Accuracy: 61.3500
Epoch 31: 100%|██████████| 782/782 [00:44<00:00, 17.64batch/s]
Avg Loss : 1.6814 Validation Loss : 1.9765 Learning Late: 0.0009 Accuracy: 55.9500
Epoch 32: 100%|██████████| 782/782 [00:44<00:00, 17.65batch/s]
Avg Loss : 1.7210 Validation Loss : 2.1125 Learning Late: 0.0009 Accuracy: 50.8500
Epoch 33: 100%|██████████| 782/782 [00:44<00:00, 17.67batch/s]
Avg Loss : 1.6685 Validation Loss : 1.8284 Learning Late: 0.0008 Accuracy: 56.5000
Epoch 34: 100%|██████████| 782/782 [00:44<00:00, 17.65batch/s]
Avg Loss : 1.6661 Validation Loss : 1.7624 Learning Late: 0.0008 Accuracy: 57.8200
Epoch 35: 100%|██████████| 782/782 [00:44<00:00, 17.60batch/s]
Avg Loss : 1.6652 Validation Loss : 1.7485 Learning Late: 0.0008 Accuracy: 56.8700
Epoch 36: 100%|██████████| 782/782 [00:44<00:00, 17.66batch/s]
Avg Loss : 1.6160 Validation Loss : 1.5243 Learning Late: 0.0008 Accuracy: 60.1700
Epoch 37: 100%|██████████| 782/782 [00:43<00:00, 18.11batch/s]
Avg Loss : 1.6170 Validation Loss : 1.7532 Learning Late: 0.0008 Accuracy: 57.4300
Epoch 38: 100%|██████████| 782/782 [00:43<00:00, 18.01batch/s]
Avg Loss : 1.6321 Validation Loss : 1.6526 Learning Late: 0.0008 Accuracy: 58.5900
Epoch 39: 100%|██████████| 782/782 [00:43<00:00, 18.01batch/s]
Avg Loss : 1.6151 Validation Loss : 1.7252 Learning Late: 0.0008 Accuracy: 59.7000
Epoch 40: 100%|██████████| 782/782 [00:43<00:00, 17.97batch/s]
Avg Loss : 1.5409 Validation Loss : 1.6421 Learning Late: 0.0007 Accuracy: 58.9000
Epoch 41: 100%|██████████| 782/782 [00:43<00:00, 18.00batch/s]
Avg Loss : 1.5681 Validation Loss : 1.7986 Learning Late: 0.0007 Accuracy: 55.2500
Epoch 42: 100%|██████████| 782/782 [00:43<00:00, 18.09batch/s]
Avg Loss : 1.5564 Validation Loss : 1.6258 Learning Late: 0.0007 Accuracy: 58.2400
Epoch 43: 100%|██████████| 782/782 [00:43<00:00, 18.08batch/s]
Avg Loss : 1.5018 Validation Loss : 1.4776 Learning Late: 0.0007 Accuracy: 61.2400
Epoch 44: 100%|██████████| 782/782 [00:43<00:00, 18.10batch/s]
Avg Loss : 1.4952 Validation Loss : 1.5506 Learning Late: 0.0007 Accuracy: 59.9500
Epoch 45: 100%|██████████| 782/782 [00:43<00:00, 18.06batch/s]
Avg Loss : 1.4741 Validation Loss : 1.5965 Learning Late: 0.0007 Accuracy: 58.6700
Epoch 46: 100%|██████████| 782/782 [00:43<00:00, 18.03batch/s]
Avg Loss : 1.4950 Validation Loss : 2.0354 Learning Late: 0.0007 Accuracy: 56.2600
Epoch 47: 100%|██████████| 782/782 [00:43<00:00, 18.10batch/s]
Avg Loss : 1.4433 Validation Loss : 2.0023 Learning Late: 0.0006 Accuracy: 52.4200
Epoch 48: 100%|██████████| 782/782 [00:43<00:00, 18.01batch/s]
Avg Loss : 1.4651 Validation Loss : 1.4368 Learning Late: 0.0006 Accuracy: 60.4600
Epoch 49: 100%|██████████| 782/782 [00:43<00:00, 18.00batch/s]
Avg Loss : 1.4157 Validation Loss : 1.7894 Learning Late: 0.0006 Accuracy: 54.9100
Epoch 50: 100%|██████████| 782/782 [00:43<00:00, 18.09batch/s]
Avg Loss : 1.4361 Validation Loss : 1.6512 Learning Late: 0.0006 Accuracy: 57.1600
Epoch 51: 100%|██████████| 782/782 [00:43<00:00, 18.07batch/s]
Avg Loss : 1.4035 Validation Loss : 1.4809 Learning Late: 0.0006 Accuracy: 59.8000
Epoch 52: 100%|██████████| 782/782 [00:43<00:00, 18.09batch/s]
Avg Loss : 1.3583 Validation Loss : 1.4562 Learning Late: 0.0006 Accuracy: 59.5800
Epoch 53: 100%|██████████| 782/782 [00:43<00:00, 18.08batch/s]
Avg Loss : 1.3638 Validation Loss : 1.4987 Learning Late: 0.0005 Accuracy: 58.4100
Epoch 54: 100%|██████████| 782/782 [00:43<00:00, 18.01batch/s]
Avg Loss : 1.3288 Validation Loss : 1.5454 Learning Late: 0.0005 Accuracy: 57.8800
Epoch 55: 100%|██████████| 782/782 [00:43<00:00, 18.08batch/s]
Avg Loss : 1.3241 Validation Loss : 1.5173 Learning Late: 0.0005 Accuracy: 58.9900
Epoch 56: 100%|██████████| 782/782 [00:43<00:00, 18.02batch/s]
Avg Loss : 1.2953 Validation Loss : 1.2959 Learning Late: 0.0005 Accuracy: 61.5000
Epoch 57: 100%|██████████| 782/782 [00:43<00:00, 18.11batch/s]
Avg Loss : 1.2720 Validation Loss : 1.4553 Learning Late: 0.0005 Accuracy: 57.0200
Epoch 58: 100%|██████████| 782/782 [00:43<00:00, 18.10batch/s]
Avg Loss : 1.2475 Validation Loss : 1.3298 Learning Late: 0.0004 Accuracy: 60.0100
Epoch 59: 100%|██████████| 782/782 [00:43<00:00, 18.01batch/s]
Avg Loss : 1.2458 Validation Loss : 1.3643 Learning Late: 0.0004 Accuracy: 60.3200
Epoch 60: 100%|██████████| 782/782 [00:43<00:00, 18.01batch/s]
Avg Loss : 1.2217 Validation Loss : 1.3263 Learning Late: 0.0004 Accuracy: 60.3600
Epoch 61: 100%|██████████| 782/782 [00:43<00:00, 18.11batch/s]
Avg Loss : 1.2103 Validation Loss : 1.3164 Learning Late: 0.0004 Accuracy: 60.0500
Epoch 62: 100%|██████████| 782/782 [00:43<00:00, 17.99batch/s]
Avg Loss : 1.1810 Validation Loss : 1.2733 Learning Late: 0.0004 Accuracy: 60.5400
Epoch 63: 100%|██████████| 782/782 [00:43<00:00, 18.00batch/s]
Avg Loss : 1.1881 Validation Loss : 1.4687 Learning Late: 0.0004 Accuracy: 56.2000
Epoch 64: 100%|██████████| 782/782 [00:43<00:00, 18.00batch/s]
Avg Loss : 1.1617 Validation Loss : 1.1539 Learning Late: 0.0003 Accuracy: 63.1400
Epoch 65: 100%|██████████| 782/782 [00:43<00:00, 18.08batch/s]
Avg Loss : 1.1521 Validation Loss : 1.1764 Learning Late: 0.0003 Accuracy: 62.2500
Epoch 66: 100%|██████████| 782/782 [00:43<00:00, 18.06batch/s]
Avg Loss : 1.1377 Validation Loss : 1.1649 Learning Late: 0.0003 Accuracy: 62.7900
Epoch 67: 100%|██████████| 782/782 [00:43<00:00, 18.00batch/s]
Avg Loss : 1.1175 Validation Loss : 1.2081 Learning Late: 0.0003 Accuracy: 62.1100
Epoch 68: 100%|██████████| 782/782 [00:43<00:00, 18.09batch/s]
Avg Loss : 1.0982 Validation Loss : 1.1738 Learning Late: 0.0003 Accuracy: 61.7600
Epoch 69: 100%|██████████| 782/782 [00:43<00:00, 18.00batch/s]
Avg Loss : 1.0996 Validation Loss : 1.1710 Learning Late: 0.0003 Accuracy: 62.1300
Epoch 70: 100%|██████████| 782/782 [00:43<00:00, 18.01batch/s]
Avg Loss : 1.0957 Validation Loss : 1.0882 Learning Late: 0.0002 Accuracy: 64.2400
Epoch 71: 100%|██████████| 782/782 [00:43<00:00, 18.04batch/s]
Avg Loss : 1.0675 Validation Loss : 1.1322 Learning Late: 0.0002 Accuracy: 62.6100
Epoch 72: 100%|██████████| 782/782 [00:43<00:00, 18.01batch/s]
Avg Loss : 1.0573 Validation Loss : 1.1955 Learning Late: 0.0002 Accuracy: 60.2300
Epoch 73: 100%|██████████| 782/782 [00:43<00:00, 18.01batch/s]
Avg Loss : 1.0429 Validation Loss : 1.1126 Learning Late: 0.0002 Accuracy: 63.0700
Epoch 74: 100%|██████████| 782/782 [00:43<00:00, 18.00batch/s]
Avg Loss : 1.0365 Validation Loss : 1.1196 Learning Late: 0.0002 Accuracy: 62.1100
Epoch 75: 100%|██████████| 782/782 [00:43<00:00, 18.01batch/s]
Avg Loss : 1.0254 Validation Loss : 1.0767 Learning Late: 0.0002 Accuracy: 63.7400
Epoch 76: 100%|██████████| 782/782 [00:43<00:00, 18.02batch/s]
Avg Loss : 1.0181 Validation Loss : 1.1012 Learning Late: 0.0002 Accuracy: 62.7100
Epoch 77: 100%|██████████| 782/782 [00:43<00:00, 18.11batch/s]
Avg Loss : 1.0050 Validation Loss : 1.0392 Learning Late: 0.0002 Accuracy: 64.9300
Epoch 78: 100%|██████████| 782/782 [00:43<00:00, 18.04batch/s]
Avg Loss : 0.9923 Validation Loss : 1.0669 Learning Late: 0.0001 Accuracy: 63.8900
Epoch 79: 100%|██████████| 782/782 [00:43<00:00, 18.00batch/s]
Avg Loss : 0.9806 Validation Loss : 1.0596 Learning Late: 0.0001 Accuracy: 64.2900
Epoch 80: 100%|██████████| 782/782 [00:43<00:00, 18.00batch/s]
Avg Loss : 0.9826 Validation Loss : 1.0552 Learning Late: 0.0001 Accuracy: 64.1300
Epoch 81: 100%|██████████| 782/782 [00:43<00:00, 18.00batch/s]
Avg Loss : 0.9702 Validation Loss : 1.0316 Learning Late: 0.0001 Accuracy: 64.9100
Epoch 82: 100%|██████████| 782/782 [00:43<00:00, 17.99batch/s]
Avg Loss : 0.9599 Validation Loss : 1.0333 Learning Late: 0.0001 Accuracy: 65.1800
Epoch 83: 100%|██████████| 782/782 [00:43<00:00, 18.02batch/s]
Avg Loss : 0.9558 Validation Loss : 1.0145 Learning Late: 0.0001 Accuracy: 65.6400
Epoch 84: 100%|██████████| 782/782 [00:43<00:00, 18.01batch/s]
Avg Loss : 0.9455 Validation Loss : 1.0195 Learning Late: 0.0001 Accuracy: 65.5100
Epoch 85: 100%|██████████| 782/782 [00:43<00:00, 18.08batch/s]
Avg Loss : 0.9364 Validation Loss : 1.0155 Learning Late: 0.0001 Accuracy: 65.4600
Epoch 86: 100%|██████████| 782/782 [00:43<00:00, 18.00batch/s]
Avg Loss : 0.9336 Validation Loss : 0.9949 Learning Late: 0.0001 Accuracy: 66.0900
Epoch 87: 100%|██████████| 782/782 [00:43<00:00, 18.00batch/s]
Avg Loss : 0.9286 Validation Loss : 1.0081 Learning Late: 0.0001 Accuracy: 65.3000
Epoch 88: 100%|██████████| 782/782 [00:43<00:00, 17.93batch/s]
Avg Loss : 0.9180 Validation Loss : 0.9946 Learning Late: 0.0000 Accuracy: 65.9100
Epoch 89: 100%|██████████| 782/782 [00:43<00:00, 17.90batch/s]
Avg Loss : 0.9146 Validation Loss : 0.9960 Learning Late: 0.0000 Accuracy: 66.0700
Epoch 90: 100%|██████████| 782/782 [00:43<00:00, 17.93batch/s]
Avg Loss : 0.9087 Validation Loss : 0.9927 Learning Late: 0.0000 Accuracy: 65.6500
Epoch 91: 100%|██████████| 782/782 [00:43<00:00, 17.91batch/s]
Avg Loss : 0.9043 Validation Loss : 0.9823 Learning Late: 0.0000 Accuracy: 66.3400
Epoch 92: 100%|██████████| 782/782 [00:43<00:00, 17.90batch/s]
Avg Loss : 0.9000 Validation Loss : 0.9807 Learning Late: 0.0000 Accuracy: 66.2900
Epoch 93: 100%|██████████| 782/782 [00:43<00:00, 17.94batch/s]
Avg Loss : 0.8964 Validation Loss : 0.9800 Learning Late: 0.0000 Accuracy: 66.2300
Epoch 94: 100%|██████████| 782/782 [00:43<00:00, 17.94batch/s]
Avg Loss : 0.8930 Validation Loss : 0.9800 Learning Late: 0.0000 Accuracy: 66.4500
Epoch 95: 100%|██████████| 782/782 [00:43<00:00, 17.87batch/s]
Avg Loss : 0.8897 Validation Loss : 0.9754 Learning Late: 0.0000 Accuracy: 66.4200
Epoch 96: 100%|██████████| 782/782 [00:43<00:00, 17.91batch/s]
Avg Loss : 0.8876 Validation Loss : 0.9734 Learning Late: 0.0000 Accuracy: 66.4600
Epoch 97: 100%|██████████| 782/782 [00:43<00:00, 17.93batch/s]
Avg Loss : 0.8851 Validation Loss : 0.9742 Learning Late: 0.0000 Accuracy: 66.5400
Epoch 98: 100%|██████████| 782/782 [00:43<00:00, 17.90batch/s]
Avg Loss : 0.8840 Validation Loss : 0.9718 Learning Late: 0.0000 Accuracy: 66.8100
Epoch 99: 100%|██████████| 782/782 [00:43<00:00, 17.90batch/s]
Avg Loss : 0.8828 Validation Loss : 0.9741 Learning Late: 0.0000 Accuracy: 66.6700
Epoch 100: 100%|██████████| 782/782 [00:43<00:00, 17.89batch/s]
Avg Loss : 0.8826 Validation Loss : 0.9720 Learning Late: 0.0000 Accuracy: 66.6700
  0%|          | 0/157 [00:00<?, ?batch/s]실제 test
100%|██████████| 157/157 [00:33<00:00,  4.63batch/s]
총 개수 : 10000
top-1 맞춘 개수 : 6667
 정확도: 66.67
top-5 맞춘 개수 : 9703
 정확도: 97.03

종료 코드 0(으)로 완료된 프로세스
