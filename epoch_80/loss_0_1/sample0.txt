C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main_with_originalres.py
Files already downloaded and verified
Files already downloaded and verified
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
Sequential                                    [1024, 64]                --
├─ResNet: 1-1                                 [1024, 64]                --
│    └─Conv2d: 2-1                            [1024, 64, 16, 16]        9,408
│    └─BatchNorm2d: 2-2                       [1024, 64, 16, 16]        128
│    └─ReLU: 2-3                              [1024, 64, 16, 16]        --
│    └─MaxPool2d: 2-4                         [1024, 64, 8, 8]          --
│    └─Sequential: 2-5                        [1024, 64, 8, 8]          --
│    │    └─BasicBlock: 3-1                   [1024, 64, 8, 8]          73,984
│    │    └─BasicBlock: 3-2                   [1024, 64, 8, 8]          73,984
│    └─Sequential: 2-6                        [1024, 128, 4, 4]         --
│    │    └─BasicBlock: 3-3                   [1024, 128, 4, 4]         230,144
│    │    └─BasicBlock: 3-4                   [1024, 128, 4, 4]         295,424
│    └─Sequential: 2-7                        [1024, 256, 2, 2]         --
│    │    └─BasicBlock: 3-5                   [1024, 256, 2, 2]         919,040
│    │    └─BasicBlock: 3-6                   [1024, 256, 2, 2]         1,180,672
│    └─Sequential: 2-8                        [1024, 512, 1, 1]         --
│    │    └─BasicBlock: 3-7                   [1024, 512, 1, 1]         3,673,088
│    │    └─BasicBlock: 3-8                   [1024, 512, 1, 1]         4,720,640
│    └─AdaptiveAvgPool2d: 2-9                 [1024, 512, 1, 1]         --
│    └─Linear: 2-10                           [1024, 64]                32,832
├─SimpleMLP: 1-2                              [1024, 64]                --
│    └─Linear: 2-11                           [1024, 64]                4,160
│    └─ReLU: 2-12                             [1024, 64]                --
│    └─Linear: 2-13                           [1024, 64]                4,160
===============================================================================================
Total params: 11,217,664
Trainable params: 11,217,664
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 37.95
===============================================================================================
Input size (MB): 12.58
Forward/backward pass size (MB): 832.05
Params size (MB): 44.87
Estimated Total Size (MB): 889.50
===============================================================================================
C:\Users\kimJuhwan\anaconda3\envs\pytorch\Lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch : 0, Avg Loss : 5.8944, Validation Loss : 4.8447
Epoch : 1, Avg Loss : 4.5601, Validation Loss : 4.0719
Epoch : 2, Avg Loss : 4.1751, Validation Loss : 3.5916
Epoch : 3, Avg Loss : 3.7288, Validation Loss : 3.6744
Epoch : 4, Avg Loss : 3.5511, Validation Loss : 4.0328
Epoch : 5, Avg Loss : 3.4156, Validation Loss : 3.1453
Epoch : 6, Avg Loss : 3.2555, Validation Loss : 3.0269
Epoch : 7, Avg Loss : 2.9273, Validation Loss : 2.8966
Epoch : 8, Avg Loss : 2.9654, Validation Loss : 2.6792
Epoch : 9, Avg Loss : 2.6199, Validation Loss : 2.8758
Epoch : 10, Avg Loss : 2.5645, Validation Loss : 3.1277
Epoch : 11, Avg Loss : 2.7383, Validation Loss : 2.7839
Epoch : 12, Avg Loss : 2.4614, Validation Loss : 2.4057
Epoch : 13, Avg Loss : 2.5971, Validation Loss : 2.8889
Epoch : 14, Avg Loss : 2.4885, Validation Loss : 2.6331
Epoch : 15, Avg Loss : 2.2853, Validation Loss : 2.0913
Epoch : 16, Avg Loss : 2.1750, Validation Loss : 2.0481
Epoch : 17, Avg Loss : 2.2559, Validation Loss : 2.2839
Epoch : 18, Avg Loss : 2.2092, Validation Loss : 2.1106
Epoch : 19, Avg Loss : 2.0193, Validation Loss : 1.6779
Epoch : 20, Avg Loss : 1.9030, Validation Loss : 2.0451
Epoch : 21, Avg Loss : 2.0165, Validation Loss : 2.1410
Epoch : 22, Avg Loss : 1.7804, Validation Loss : 2.3672
Epoch : 23, Avg Loss : 1.7367, Validation Loss : 1.7187
Epoch : 24, Avg Loss : 1.7200, Validation Loss : 1.9828
Epoch : 25, Avg Loss : 1.6480, Validation Loss : 1.7435
Epoch : 26, Avg Loss : 1.5190, Validation Loss : 1.3438
Epoch : 27, Avg Loss : 1.7051, Validation Loss : 2.1433
Epoch : 28, Avg Loss : 1.7896, Validation Loss : 1.5551
Epoch : 29, Avg Loss : 1.8455, Validation Loss : 1.6666
FG 학습 완료. 이제 F의 output을 실제 dataset의 label과 연결.
Epoch : 0, Avg Loss : 2.3003 Validation Loss : 2.2857
Epoch : 1, Avg Loss : 2.2691 Validation Loss : 2.2553
Epoch : 2, Avg Loss : 2.2395 Validation Loss : 2.2263
Epoch : 3, Avg Loss : 2.2115 Validation Loss : 2.1981
Epoch : 4, Avg Loss : 2.1850 Validation Loss : 2.1723
Epoch : 5, Avg Loss : 2.1597 Validation Loss : 2.1479
Epoch : 6, Avg Loss : 2.1359 Validation Loss : 2.1237
Epoch : 7, Avg Loss : 2.1137 Validation Loss : 2.1020
Epoch : 8, Avg Loss : 2.0923 Validation Loss : 2.0806
Epoch : 9, Avg Loss : 2.0723 Validation Loss : 2.0612
Epoch : 10, Avg Loss : 2.0530 Validation Loss : 2.0423
Epoch : 11, Avg Loss : 2.0352 Validation Loss : 2.0250
Epoch : 12, Avg Loss : 2.0182 Validation Loss : 2.0088
Epoch : 13, Avg Loss : 2.0019 Validation Loss : 1.9928
Epoch : 14, Avg Loss : 1.9868 Validation Loss : 1.9764
Epoch : 15, Avg Loss : 1.9725 Validation Loss : 1.9625
Epoch : 16, Avg Loss : 1.9584 Validation Loss : 1.9490
Epoch : 17, Avg Loss : 1.9455 Validation Loss : 1.9359
Epoch : 18, Avg Loss : 1.9334 Validation Loss : 1.9237
Epoch : 19, Avg Loss : 1.9220 Validation Loss : 1.9121
Epoch : 20, Avg Loss : 1.9109 Validation Loss : 1.9013
Epoch : 21, Avg Loss : 1.9002 Validation Loss : 1.8918
Epoch : 22, Avg Loss : 1.8903 Validation Loss : 1.8815
Epoch : 23, Avg Loss : 1.8808 Validation Loss : 1.8712
Epoch : 24, Avg Loss : 1.8720 Validation Loss : 1.8638
Epoch : 25, Avg Loss : 1.8634 Validation Loss : 1.8535
Epoch : 26, Avg Loss : 1.8552 Validation Loss : 1.8465
Epoch : 27, Avg Loss : 1.8480 Validation Loss : 1.8398
Epoch : 28, Avg Loss : 1.8402 Validation Loss : 1.8298
Epoch : 29, Avg Loss : 1.8336 Validation Loss : 1.8247
실제 test
총 개수 : 10000
 맞춘 개수 : 3996
 정확도: 39.96
 찍었을 때의 정확도 : 10

종료 코드 0(으)로 완료된 프로세스
