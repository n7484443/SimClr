C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main_with_originalres.py
Files already downloaded and verified
Files already downloaded and verified
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
Sequential                                    [512, 64]                 --
├─ResNet: 1-1                                 [512, 64]                 --
│    └─Conv2d: 2-1                            [512, 64, 16, 16]         9,408
│    └─BatchNorm2d: 2-2                       [512, 64, 16, 16]         128
│    └─ReLU: 2-3                              [512, 64, 16, 16]         --
│    └─MaxPool2d: 2-4                         [512, 64, 8, 8]           --
│    └─Sequential: 2-5                        [512, 64, 8, 8]           --
│    │    └─BasicBlock: 3-1                   [512, 64, 8, 8]           73,984
│    │    └─BasicBlock: 3-2                   [512, 64, 8, 8]           73,984
│    └─Sequential: 2-6                        [512, 128, 4, 4]          --
│    │    └─BasicBlock: 3-3                   [512, 128, 4, 4]          230,144
│    │    └─BasicBlock: 3-4                   [512, 128, 4, 4]          295,424
│    └─Sequential: 2-7                        [512, 256, 2, 2]          --
│    │    └─BasicBlock: 3-5                   [512, 256, 2, 2]          919,040
│    │    └─BasicBlock: 3-6                   [512, 256, 2, 2]          1,180,672
│    └─Sequential: 2-8                        [512, 512, 1, 1]          --
│    │    └─BasicBlock: 3-7                   [512, 512, 1, 1]          3,673,088
│    │    └─BasicBlock: 3-8                   [512, 512, 1, 1]          4,720,640
│    └─AdaptiveAvgPool2d: 2-9                 [512, 512, 1, 1]          --
│    └─Linear: 2-10                           [512, 64]                 32,832
├─SimpleMLP: 1-2                              [512, 64]                 --
│    └─Linear: 2-11                           [512, 64]                 4,160
│    └─ReLU: 2-12                             [512, 64]                 --
│    └─Linear: 2-13                           [512, 64]                 4,160
===============================================================================================
Total params: 11,217,664
Trainable params: 11,217,664
Non-trainable params: 0
Total mult-adds (G): 18.98
===============================================================================================
Input size (MB): 6.29
Forward/backward pass size (MB): 416.02
Params size (MB): 44.87
Estimated Total Size (MB): 467.18
===============================================================================================
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch : 0, Avg Loss : 5.0403 lr: 0.00100000
Epoch : 1, Avg Loss : 4.5463 lr: 0.00095000
Epoch : 2, Avg Loss : 4.3761 lr: 0.00090250
Epoch : 3, Avg Loss : 4.3946 lr: 0.00085737
Epoch : 4, Avg Loss : 4.2860 lr: 0.00081451
Epoch : 5, Avg Loss : 4.2075 lr: 0.00077378
Epoch : 6, Avg Loss : 4.1851 lr: 0.00073509
Epoch : 7, Avg Loss : 4.1436 lr: 0.00069834
Epoch : 8, Avg Loss : 4.1340 lr: 0.00066342
Epoch : 9, Avg Loss : 4.0210 lr: 0.00063025
Epoch : 10, Avg Loss : 4.0177 lr: 0.00059874
Epoch : 11, Avg Loss : 3.9478 lr: 0.00056880
Epoch : 12, Avg Loss : 3.9442 lr: 0.00054036
Epoch : 13, Avg Loss : 3.8613 lr: 0.00051334
Epoch : 14, Avg Loss : 3.8581 lr: 0.00048767
Epoch : 15, Avg Loss : 3.8433 lr: 0.00046329
Epoch : 16, Avg Loss : 3.7831 lr: 0.00044013
Epoch : 17, Avg Loss : 3.7847 lr: 0.00041812
Epoch : 18, Avg Loss : 3.7718 lr: 0.00039721
Epoch : 19, Avg Loss : 3.7367 lr: 0.00037735
Epoch : 20, Avg Loss : 3.7937 lr: 0.00035849
Epoch : 21, Avg Loss : 3.7956 lr: 0.00034056
Epoch : 22, Avg Loss : 3.7362 lr: 0.00032353
Epoch : 23, Avg Loss : 3.7344 lr: 0.00030736
Epoch : 24, Avg Loss : 3.7260 lr: 0.00029199
Epoch : 25, Avg Loss : 3.6633 lr: 0.00027739
Epoch : 26, Avg Loss : 3.6947 lr: 0.00026352
Epoch : 27, Avg Loss : 3.7352 lr: 0.00025034
Epoch : 28, Avg Loss : 3.6584 lr: 0.00023783
Epoch : 29, Avg Loss : 3.6709 lr: 0.00022594
Epoch : 30, Avg Loss : 3.6472 lr: 0.00021464
Epoch : 31, Avg Loss : 3.6503 lr: 0.00020391
Epoch : 32, Avg Loss : 3.5874 lr: 0.00019371
Epoch : 33, Avg Loss : 3.6450 lr: 0.00018403
Epoch : 34, Avg Loss : 3.6043 lr: 0.00017482
Epoch : 35, Avg Loss : 3.5982 lr: 0.00016608
Epoch : 36, Avg Loss : 3.6157 lr: 0.00015778
Epoch : 37, Avg Loss : 3.5812 lr: 0.00014989
Epoch : 38, Avg Loss : 3.6118 lr: 0.00014240
Epoch : 39, Avg Loss : 3.5938 lr: 0.00013528
Epoch : 40, Avg Loss : 3.6061 lr: 0.00012851
Epoch : 41, Avg Loss : 3.5845 lr: 0.00012209
Epoch : 42, Avg Loss : 3.5817 lr: 0.00011598
Epoch : 43, Avg Loss : 3.6319 lr: 0.00011018
Epoch : 44, Avg Loss : 3.5767 lr: 0.00010467
Epoch : 45, Avg Loss : 3.5326 lr: 0.00009944
Epoch : 46, Avg Loss : 3.5381 lr: 0.00009447
Epoch : 47, Avg Loss : 3.5598 lr: 0.00008974
Epoch : 48, Avg Loss : 3.5639 lr: 0.00008526
Epoch : 49, Avg Loss : 3.5556 lr: 0.00008099
Epoch : 50, Avg Loss : 3.5674 lr: 0.00007694
Epoch : 51, Avg Loss : 3.5261 lr: 0.00007310
Epoch : 52, Avg Loss : 3.5504 lr: 0.00006944
Epoch : 53, Avg Loss : 3.5132 lr: 0.00006597
Epoch : 54, Avg Loss : 3.5432 lr: 0.00006267
Epoch : 55, Avg Loss : 3.5266 lr: 0.00005954
Epoch : 56, Avg Loss : 3.5218 lr: 0.00005656
Epoch : 57, Avg Loss : 3.5017 lr: 0.00005373
Epoch : 58, Avg Loss : 3.5286 lr: 0.00005105
Epoch : 59, Avg Loss : 3.5098 lr: 0.00004849
Epoch : 60, Avg Loss : 3.5162 lr: 0.00004607
Epoch : 61, Avg Loss : 3.5363 lr: 0.00004377
Epoch : 62, Avg Loss : 3.4893 lr: 0.00004158
Epoch : 63, Avg Loss : 3.4970 lr: 0.00003950
Epoch : 64, Avg Loss : 3.4908 lr: 0.00003752
Epoch : 65, Avg Loss : 3.5244 lr: 0.00003565
Epoch : 66, Avg Loss : 3.4767 lr: 0.00003387
Epoch : 67, Avg Loss : 3.4952 lr: 0.00003217
Epoch : 68, Avg Loss : 3.5104 lr: 0.00003056
Epoch : 69, Avg Loss : 3.5373 lr: 0.00002904
Epoch : 70, Avg Loss : 3.4856 lr: 0.00002758
Epoch : 71, Avg Loss : 3.5265 lr: 0.00002620
Epoch : 72, Avg Loss : 3.4852 lr: 0.00002489
Epoch : 73, Avg Loss : 3.4781 lr: 0.00002365
Epoch : 74, Avg Loss : 3.4806 lr: 0.00002247
Epoch : 75, Avg Loss : 3.5001 lr: 0.00002134
Epoch : 76, Avg Loss : 3.4979 lr: 0.00002028
Epoch : 77, Avg Loss : 3.5058 lr: 0.00001926
Epoch : 78, Avg Loss : 3.4981 lr: 0.00001830
Epoch : 79, Avg Loss : 3.4980 lr: 0.00001738
Epoch : 80, Avg Loss : 3.4862 lr: 0.00001652
Epoch : 81, Avg Loss : 3.5445 lr: 0.00001569
Epoch : 82, Avg Loss : 3.4826 lr: 0.00001491
Epoch : 83, Avg Loss : 3.4883 lr: 0.00001416
Epoch : 84, Avg Loss : 3.4974 lr: 0.00001345
Epoch : 85, Avg Loss : 3.4803 lr: 0.00001278
Epoch : 86, Avg Loss : 3.5108 lr: 0.00001214
Epoch : 87, Avg Loss : 3.4705 lr: 0.00001153
Epoch : 88, Avg Loss : 3.5241 lr: 0.00001096
Epoch : 89, Avg Loss : 3.4780 lr: 0.00001041
Epoch : 90, Avg Loss : 3.5090 lr: 0.00000989
Epoch : 91, Avg Loss : 3.4691 lr: 0.00000939
Epoch : 92, Avg Loss : 3.4683 lr: 0.00000892
Epoch : 93, Avg Loss : 3.4918 lr: 0.00000848
Epoch : 94, Avg Loss : 3.4832 lr: 0.00000805
Epoch : 95, Avg Loss : 3.5076 lr: 0.00000765
Epoch : 96, Avg Loss : 3.4732 lr: 0.00000727
Epoch : 97, Avg Loss : 3.4899 lr: 0.00000691
Epoch : 98, Avg Loss : 3.4744 lr: 0.00000656
Epoch : 99, Avg Loss : 3.4578 lr: 0.00000623
FG 학습 완료. 이제 F의 output을 실제 dataset의 label과 연결.
Epoch : 0, Avg Loss : 2.1544 lr: 0.00100000
Epoch : 1, Avg Loss : 1.8889 lr: 0.00095000
Epoch : 2, Avg Loss : 1.7659 lr: 0.00090250
Epoch : 3, Avg Loss : 1.7039 lr: 0.00085737
Epoch : 4, Avg Loss : 1.6688 lr: 0.00081451
Epoch : 5, Avg Loss : 1.6478 lr: 0.00077378
Epoch : 6, Avg Loss : 1.6326 lr: 0.00073509
Epoch : 7, Avg Loss : 1.6225 lr: 0.00069834
Epoch : 8, Avg Loss : 1.6152 lr: 0.00066342
Epoch : 9, Avg Loss : 1.6100 lr: 0.00063025
Epoch : 10, Avg Loss : 1.6057 lr: 0.00059874
Epoch : 11, Avg Loss : 1.6006 lr: 0.00056880
Epoch : 12, Avg Loss : 1.5981 lr: 0.00054036
Epoch : 13, Avg Loss : 1.5947 lr: 0.00051334
Epoch : 14, Avg Loss : 1.5927 lr: 0.00048767
Epoch : 15, Avg Loss : 1.5908 lr: 0.00046329
Epoch : 16, Avg Loss : 1.5899 lr: 0.00044013
Epoch : 17, Avg Loss : 1.5864 lr: 0.00041812
Epoch : 18, Avg Loss : 1.5866 lr: 0.00039721
Epoch : 19, Avg Loss : 1.5844 lr: 0.00037735
Epoch : 20, Avg Loss : 1.5843 lr: 0.00035849
Epoch : 21, Avg Loss : 1.5836 lr: 0.00034056
Epoch : 22, Avg Loss : 1.5821 lr: 0.00032353
Epoch : 23, Avg Loss : 1.5831 lr: 0.00030736
Epoch : 24, Avg Loss : 1.5825 lr: 0.00029199
Epoch : 25, Avg Loss : 1.5808 lr: 0.00027739
Epoch : 26, Avg Loss : 1.5806 lr: 0.00026352
Epoch : 27, Avg Loss : 1.5801 lr: 0.00025034
Epoch : 28, Avg Loss : 1.5796 lr: 0.00023783
Epoch : 29, Avg Loss : 1.5792 lr: 0.00022594
Epoch : 30, Avg Loss : 1.5785 lr: 0.00021464
Epoch : 31, Avg Loss : 1.5781 lr: 0.00020391
Epoch : 32, Avg Loss : 1.5775 lr: 0.00019371
Epoch : 33, Avg Loss : 1.5778 lr: 0.00018403
Epoch : 34, Avg Loss : 1.5779 lr: 0.00017482
Epoch : 35, Avg Loss : 1.5773 lr: 0.00016608
Epoch : 36, Avg Loss : 1.5773 lr: 0.00015778
Epoch : 37, Avg Loss : 1.5773 lr: 0.00014989
Epoch : 38, Avg Loss : 1.5770 lr: 0.00014240
Epoch : 39, Avg Loss : 1.5770 lr: 0.00013528
Epoch : 40, Avg Loss : 1.5768 lr: 0.00012851
Epoch : 41, Avg Loss : 1.5758 lr: 0.00012209
Epoch : 42, Avg Loss : 1.5765 lr: 0.00011598
Epoch : 43, Avg Loss : 1.5756 lr: 0.00011018
Epoch : 44, Avg Loss : 1.5762 lr: 0.00010467
Epoch : 45, Avg Loss : 1.5756 lr: 0.00009944
Epoch : 46, Avg Loss : 1.5763 lr: 0.00009447
Epoch : 47, Avg Loss : 1.5761 lr: 0.00008974
Epoch : 48, Avg Loss : 1.5761 lr: 0.00008526
Epoch : 49, Avg Loss : 1.5756 lr: 0.00008099
Epoch : 50, Avg Loss : 1.5737 lr: 0.00007694
Epoch : 51, Avg Loss : 1.5756 lr: 0.00007310
Epoch : 52, Avg Loss : 1.5750 lr: 0.00006944
Epoch : 53, Avg Loss : 1.5754 lr: 0.00006597
Epoch : 54, Avg Loss : 1.5749 lr: 0.00006267
Epoch : 55, Avg Loss : 1.5755 lr: 0.00005954
Epoch : 56, Avg Loss : 1.5747 lr: 0.00005656
Epoch : 57, Avg Loss : 1.5748 lr: 0.00005373
Epoch : 58, Avg Loss : 1.5746 lr: 0.00005105
Epoch : 59, Avg Loss : 1.5744 lr: 0.00004849
Epoch : 60, Avg Loss : 1.5744 lr: 0.00004607
Epoch : 61, Avg Loss : 1.5735 lr: 0.00004377
Epoch : 62, Avg Loss : 1.5743 lr: 0.00004158
Epoch : 63, Avg Loss : 1.5737 lr: 0.00003950
Epoch : 64, Avg Loss : 1.5742 lr: 0.00003752
Epoch : 65, Avg Loss : 1.5743 lr: 0.00003565
Epoch : 66, Avg Loss : 1.5760 lr: 0.00003387
Epoch : 67, Avg Loss : 1.5736 lr: 0.00003217
Epoch : 68, Avg Loss : 1.5734 lr: 0.00003056
Epoch : 69, Avg Loss : 1.5735 lr: 0.00002904
Epoch : 70, Avg Loss : 1.5739 lr: 0.00002758
Epoch : 71, Avg Loss : 1.5738 lr: 0.00002620
Epoch : 72, Avg Loss : 1.5737 lr: 0.00002489
Epoch : 73, Avg Loss : 1.5734 lr: 0.00002365
Epoch : 74, Avg Loss : 1.5728 lr: 0.00002247
Epoch : 75, Avg Loss : 1.5737 lr: 0.00002134
Epoch : 76, Avg Loss : 1.5737 lr: 0.00002028
Epoch : 77, Avg Loss : 1.5738 lr: 0.00001926
Epoch : 78, Avg Loss : 1.5741 lr: 0.00001830
Epoch : 79, Avg Loss : 1.5738 lr: 0.00001738
Epoch : 80, Avg Loss : 1.5735 lr: 0.00001652
Epoch : 81, Avg Loss : 1.5742 lr: 0.00001569
Epoch : 82, Avg Loss : 1.5738 lr: 0.00001491
Epoch : 83, Avg Loss : 1.5744 lr: 0.00001416
Epoch : 84, Avg Loss : 1.5732 lr: 0.00001345
Epoch : 85, Avg Loss : 1.5733 lr: 0.00001278
Epoch : 86, Avg Loss : 1.5737 lr: 0.00001214
Epoch : 87, Avg Loss : 1.5738 lr: 0.00001153
Epoch : 88, Avg Loss : 1.5749 lr: 0.00001096
Epoch : 89, Avg Loss : 1.5731 lr: 0.00001041
Epoch : 90, Avg Loss : 1.5748 lr: 0.00000989
Epoch : 91, Avg Loss : 1.5738 lr: 0.00000939
Epoch : 92, Avg Loss : 1.5746 lr: 0.00000892
Epoch : 93, Avg Loss : 1.5738 lr: 0.00000848
Epoch : 94, Avg Loss : 1.5732 lr: 0.00000805
Epoch : 95, Avg Loss : 1.5739 lr: 0.00000765
Epoch : 96, Avg Loss : 1.5731 lr: 0.00000727
Epoch : 97, Avg Loss : 1.5737 lr: 0.00000691
Epoch : 98, Avg Loss : 1.5722 lr: 0.00000656
Epoch : 99, Avg Loss : 1.5731 lr: 0.00000623
실제 test
총 개수 : 10000
 맞춘 개수 : 4233
 정확도: 42.33
 찍었을 때의 정확도 : 10

종료 코드 0(으)로 완료된 프로세스
