C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main.py
Files already downloaded and verified
Files already downloaded and verified
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Resnet                                   [1000, 32]                --
├─Sequential: 1-1                        [1000, 16, 32, 32]        --
│    └─Conv2d: 2-1                       [1000, 16, 32, 32]        432
│    └─BatchNorm2d: 2-2                  [1000, 16, 32, 32]        32
│    └─ReLU: 2-3                         [1000, 16, 32, 32]        --
├─Sequential: 1-2                        [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-4                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-1              [1000, 32, 16, 16]        13,952
│    │    └─Sequential: 3-2              [1000, 32, 16, 16]        576
│    │    └─ReLU: 3-3                    [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-5                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-4              [1000, 32, 16, 16]        18,560
│    │    └─Sequential: 3-5              [1000, 32, 16, 16]        --
│    │    └─ReLU: 3-6                    [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-6                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-7              [1000, 32, 16, 16]        18,560
│    │    └─Sequential: 3-8              [1000, 32, 16, 16]        --
│    │    └─ReLU: 3-9                    [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-7                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-10             [1000, 32, 16, 16]        18,560
│    │    └─Sequential: 3-11             [1000, 32, 16, 16]        --
│    │    └─ReLU: 3-12                   [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-8                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-13             [1000, 32, 16, 16]        18,560
│    │    └─Sequential: 3-14             [1000, 32, 16, 16]        --
│    │    └─ReLU: 3-15                   [1000, 32, 16, 16]        --
├─Sequential: 1-3                        [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-9                  [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-16             [1000, 64, 8, 8]          55,552
│    │    └─Sequential: 3-17             [1000, 64, 8, 8]          2,176
│    │    └─ReLU: 3-18                   [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-10                 [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-19             [1000, 64, 8, 8]          73,984
│    │    └─Sequential: 3-20             [1000, 64, 8, 8]          --
│    │    └─ReLU: 3-21                   [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-11                 [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-22             [1000, 64, 8, 8]          73,984
│    │    └─Sequential: 3-23             [1000, 64, 8, 8]          --
│    │    └─ReLU: 3-24                   [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-12                 [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-25             [1000, 64, 8, 8]          73,984
│    │    └─Sequential: 3-26             [1000, 64, 8, 8]          --
│    │    └─ReLU: 3-27                   [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-13                 [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-28             [1000, 64, 8, 8]          73,984
│    │    └─Sequential: 3-29             [1000, 64, 8, 8]          --
│    │    └─ReLU: 3-30                   [1000, 64, 8, 8]          --
├─Sequential: 1-4                        [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-14                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-31             [1000, 128, 4, 4]         221,696
│    │    └─Sequential: 3-32             [1000, 128, 4, 4]         8,448
│    │    └─ReLU: 3-33                   [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-15                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-34             [1000, 128, 4, 4]         295,424
│    │    └─Sequential: 3-35             [1000, 128, 4, 4]         --
│    │    └─ReLU: 3-36                   [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-16                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-37             [1000, 128, 4, 4]         295,424
│    │    └─Sequential: 3-38             [1000, 128, 4, 4]         --
│    │    └─ReLU: 3-39                   [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-17                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-40             [1000, 128, 4, 4]         295,424
│    │    └─Sequential: 3-41             [1000, 128, 4, 4]         --
│    │    └─ReLU: 3-42                   [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-18                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-43             [1000, 128, 4, 4]         295,424
│    │    └─Sequential: 3-44             [1000, 128, 4, 4]         --
│    │    └─ReLU: 3-45                   [1000, 128, 4, 4]         --
├─AdaptiveAvgPool2d: 1-5                 [1000, 128, 1, 1]         --
├─Sequential: 1-6                        [1000, 32]                --
│    └─Flatten: 2-19                     [1000, 128]               --
│    └─Linear: 2-20                      [1000, 32]                4,128
==========================================================================================
Total params: 1,858,864
Trainable params: 1,858,864
Non-trainable params: 0
Total mult-adds (G): 68.08
==========================================================================================
Input size (MB): 12.29
Forward/backward pass size (MB): 2785.54
Params size (MB): 7.44
Estimated Total Size (MB): 2805.26
==========================================================================================
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch : 0, Avg Loss : 4.4168
Epoch : 1, Avg Loss : 4.1081
Epoch : 2, Avg Loss : 4.0941
Epoch : 3, Avg Loss : 3.9636
Epoch : 4, Avg Loss : 4.0436
Epoch : 5, Avg Loss : 3.9739
Epoch : 6, Avg Loss : 3.9754
Epoch : 7, Avg Loss : 3.9594
Epoch : 8, Avg Loss : 3.9885
Epoch : 9, Avg Loss : 3.9521
Epoch : 10, Avg Loss : 4.0099
Epoch : 11, Avg Loss : 3.9878
Epoch : 12, Avg Loss : 3.9652
Epoch : 13, Avg Loss : 3.9772
Epoch : 14, Avg Loss : 3.9253
Epoch : 15, Avg Loss : 3.9087
Epoch : 16, Avg Loss : 3.9689
Epoch : 17, Avg Loss : 3.9052
Epoch : 18, Avg Loss : 3.9631
Epoch : 19, Avg Loss : 3.8585
Epoch : 20, Avg Loss : 3.9051
Epoch : 21, Avg Loss : 3.9798
Epoch : 22, Avg Loss : 3.9499
Epoch : 23, Avg Loss : 3.9516
Epoch : 24, Avg Loss : 3.9755
Epoch : 25, Avg Loss : 3.9872
Epoch : 26, Avg Loss : 3.8790
Epoch : 27, Avg Loss : 3.9748
Epoch : 28, Avg Loss : 3.9537
Epoch : 29, Avg Loss : 3.8884
FG 학습 완료. 이제 FG의 output을 실제 dataset의 label과 연결.
Epoch : 0, Avg Loss : 1.9664
Epoch : 1, Avg Loss : 1.9209
Epoch : 2, Avg Loss : 1.9111
Epoch : 3, Avg Loss : 1.9070
Epoch : 4, Avg Loss : 1.9042
Epoch : 5, Avg Loss : 1.9020
Epoch : 6, Avg Loss : 1.9008
Epoch : 7, Avg Loss : 1.8989
Epoch : 8, Avg Loss : 1.8977
Epoch : 9, Avg Loss : 1.8963
Epoch : 10, Avg Loss : 1.8951
Epoch : 11, Avg Loss : 1.8932
Epoch : 12, Avg Loss : 1.8925
Epoch : 13, Avg Loss : 1.8915
Epoch : 14, Avg Loss : 1.8901
Epoch : 15, Avg Loss : 1.8893
Epoch : 16, Avg Loss : 1.8887
Epoch : 17, Avg Loss : 1.8875
Epoch : 18, Avg Loss : 1.8872
Epoch : 19, Avg Loss : 1.8864
Epoch : 20, Avg Loss : 1.8854
Epoch : 21, Avg Loss : 1.8844
Epoch : 22, Avg Loss : 1.8833
Epoch : 23, Avg Loss : 1.8836
Epoch : 24, Avg Loss : 1.8824
Epoch : 25, Avg Loss : 1.8813
Epoch : 26, Avg Loss : 1.8806
Epoch : 27, Avg Loss : 1.8803
Epoch : 28, Avg Loss : 1.8798
Epoch : 29, Avg Loss : 1.8792
실제 test
총 개수 : 10000
 맞춘 개수 : 2950
 정확도: 29
 찍었을 때의 정확도 : 10

종료 코드 0(으)로 완료된 프로세스
