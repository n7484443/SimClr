C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main.py
Files already downloaded and verified
Files already downloaded and verified
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
Sequential                                    [64, 32]                  --
├─Resnet: 1-1                                 [64, 32]                  --
│    └─Conv2d: 2-1                            [64, 64, 16, 16]          9,408
│    └─Sequential: 2-2                        [64, 64, 8, 8]            --
│    │    └─MaxPool2d: 3-1                    [64, 64, 8, 8]            --
│    │    └─ResnetBlock: 3-2                  [64, 64, 8, 8]            73,984
│    │    └─ResnetBlock: 3-3                  [64, 64, 8, 8]            73,984
│    └─Sequential: 2-3                        [64, 128, 8, 8]           --
│    │    └─ResnetBlock: 3-4                  [64, 128, 8, 8]           230,144
│    │    └─ResnetBlock: 3-5                  [64, 128, 8, 8]           295,424
│    │    └─ResnetBlock: 3-6                  [64, 128, 8, 8]           295,424
│    └─Sequential: 2-4                        [64, 256, 8, 8]           --
│    │    └─ResnetBlock: 3-7                  [64, 256, 8, 8]           919,040
│    │    └─ResnetBlock: 3-8                  [64, 256, 8, 8]           1,180,672
│    │    └─ResnetBlock: 3-9                  [64, 256, 8, 8]           1,180,672
│    └─Sequential: 2-5                        [64, 512, 8, 8]           --
│    │    └─ResnetBlock: 3-10                 [64, 512, 8, 8]           3,673,088
│    │    └─ResnetBlock: 3-11                 [64, 512, 8, 8]           4,720,640
│    │    └─ResnetBlock: 3-12                 [64, 512, 8, 8]           4,720,640
│    └─AdaptiveAvgPool2d: 2-6                 [64, 512, 1, 1]           --
│    └─Sequential: 2-7                        [64, 32]                  --
│    │    └─Flatten: 3-13                     [64, 512]                 --
│    │    └─Linear: 3-14                      [64, 32]                  16,416
├─SimpleMLP: 1-2                              [64, 32]                  --
│    └─Linear: 2-8                            [64, 32]                  1,056
│    └─ReLU: 2-9                              [64, 32]                  --
===============================================================================================
Total params: 17,390,592
Trainable params: 17,390,592
Non-trainable params: 0
Total mult-adds (G): 71.22
===============================================================================================
Input size (MB): 0.79
Forward/backward pass size (MB): 436.24
Params size (MB): 69.56
Estimated Total Size (MB): 506.59
===============================================================================================
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch : 0, Avg Loss : 3.8089
Epoch : 1, Avg Loss : 3.7774
Epoch : 2, Avg Loss : 3.7670
Epoch : 3, Avg Loss : 3.7549
Epoch : 4, Avg Loss : 3.7427
Epoch : 5, Avg Loss : 3.7286
Epoch : 6, Avg Loss : 3.7241
Epoch : 7, Avg Loss : 3.7229
Epoch : 8, Avg Loss : 3.7045
Epoch : 9, Avg Loss : 3.7071
FG 학습 완료. 이제 FG의 output을 실제 dataset의 label과 연결.
Epoch : 0, Avg Loss : 2.1641
Epoch : 1, Avg Loss : 2.0804
Epoch : 2, Avg Loss : 2.0701
Epoch : 3, Avg Loss : 2.0666
Epoch : 4, Avg Loss : 2.0635
Epoch : 5, Avg Loss : 2.0617
Epoch : 6, Avg Loss : 2.0612
Epoch : 7, Avg Loss : 2.0581
Epoch : 8, Avg Loss : 2.0590
Epoch : 9, Avg Loss : 2.0575
Epoch : 10, Avg Loss : 2.0581
Epoch : 11, Avg Loss : 2.0554
Epoch : 12, Avg Loss : 2.0569
Epoch : 13, Avg Loss : 2.0551
Epoch : 14, Avg Loss : 2.0554
Epoch : 15, Avg Loss : 2.0543
Epoch : 16, Avg Loss : 2.0540
Epoch : 17, Avg Loss : 2.0551
Epoch : 18, Avg Loss : 2.0548
Epoch : 19, Avg Loss : 2.0541
실제 test
총 개수 : 50000
 맞춘 개수 : 12319
 정확도: 24
 찍었을 때의 정확도 : 10