C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main.py
Files already downloaded and verified
Files already downloaded and verified
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Resnet                                   [1000, 32]                --
├─Sequential: 1-1                        [1000, 16, 32, 32]        --
│    └─Conv2d: 2-1                       [1000, 16, 32, 32]        432
│    └─BatchNorm2d: 2-2                  [1000, 16, 32, 32]        32
│    └─ReLU: 2-3                         [1000, 16, 32, 32]        --
├─Sequential: 1-2                        [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-4                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-1              [1000, 32, 16, 16]        13,952
│    │    └─Sequential: 3-2              [1000, 32, 16, 16]        576
│    │    └─ReLU: 3-3                    [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-5                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-4              [1000, 32, 16, 16]        18,560
│    │    └─Sequential: 3-5              [1000, 32, 16, 16]        --
│    │    └─ReLU: 3-6                    [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-6                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-7              [1000, 32, 16, 16]        18,560
│    │    └─Sequential: 3-8              [1000, 32, 16, 16]        --
│    │    └─ReLU: 3-9                    [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-7                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-10             [1000, 32, 16, 16]        18,560
│    │    └─Sequential: 3-11             [1000, 32, 16, 16]        --
│    │    └─ReLU: 3-12                   [1000, 32, 16, 16]        --
│    └─ResnetBlock: 2-8                  [1000, 32, 16, 16]        --
│    │    └─Sequential: 3-13             [1000, 32, 16, 16]        18,560
│    │    └─Sequential: 3-14             [1000, 32, 16, 16]        --
│    │    └─ReLU: 3-15                   [1000, 32, 16, 16]        --
├─Sequential: 1-3                        [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-9                  [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-16             [1000, 64, 8, 8]          55,552
│    │    └─Sequential: 3-17             [1000, 64, 8, 8]          2,176
│    │    └─ReLU: 3-18                   [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-10                 [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-19             [1000, 64, 8, 8]          73,984
│    │    └─Sequential: 3-20             [1000, 64, 8, 8]          --
│    │    └─ReLU: 3-21                   [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-11                 [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-22             [1000, 64, 8, 8]          73,984
│    │    └─Sequential: 3-23             [1000, 64, 8, 8]          --
│    │    └─ReLU: 3-24                   [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-12                 [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-25             [1000, 64, 8, 8]          73,984
│    │    └─Sequential: 3-26             [1000, 64, 8, 8]          --
│    │    └─ReLU: 3-27                   [1000, 64, 8, 8]          --
│    └─ResnetBlock: 2-13                 [1000, 64, 8, 8]          --
│    │    └─Sequential: 3-28             [1000, 64, 8, 8]          73,984
│    │    └─Sequential: 3-29             [1000, 64, 8, 8]          --
│    │    └─ReLU: 3-30                   [1000, 64, 8, 8]          --
├─Sequential: 1-4                        [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-14                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-31             [1000, 128, 4, 4]         221,696
│    │    └─Sequential: 3-32             [1000, 128, 4, 4]         8,448
│    │    └─ReLU: 3-33                   [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-15                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-34             [1000, 128, 4, 4]         295,424
│    │    └─Sequential: 3-35             [1000, 128, 4, 4]         --
│    │    └─ReLU: 3-36                   [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-16                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-37             [1000, 128, 4, 4]         295,424
│    │    └─Sequential: 3-38             [1000, 128, 4, 4]         --
│    │    └─ReLU: 3-39                   [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-17                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-40             [1000, 128, 4, 4]         295,424
│    │    └─Sequential: 3-41             [1000, 128, 4, 4]         --
│    │    └─ReLU: 3-42                   [1000, 128, 4, 4]         --
│    └─ResnetBlock: 2-18                 [1000, 128, 4, 4]         --
│    │    └─Sequential: 3-43             [1000, 128, 4, 4]         295,424
│    │    └─Sequential: 3-44             [1000, 128, 4, 4]         --
│    │    └─ReLU: 3-45                   [1000, 128, 4, 4]         --
├─AdaptiveAvgPool2d: 1-5                 [1000, 128, 1, 1]         --
├─Sequential: 1-6                        [1000, 32]                --
│    └─Flatten: 2-19                     [1000, 128]               --
│    └─Linear: 2-20                      [1000, 32]                4,128
==========================================================================================
Total params: 1,858,864
Trainable params: 1,858,864
Non-trainable params: 0
Total mult-adds (G): 68.08
==========================================================================================
Input size (MB): 12.29
Forward/backward pass size (MB): 2785.54
Params size (MB): 7.44
Estimated Total Size (MB): 2805.26
==========================================================================================
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch : 0, Avg Loss : 4.2936
Epoch : 1, Avg Loss : 4.0730
Epoch : 2, Avg Loss : 3.9872
Epoch : 3, Avg Loss : 3.8871
Epoch : 4, Avg Loss : 4.0233
Epoch : 5, Avg Loss : 3.8973
Epoch : 6, Avg Loss : 3.9399
Epoch : 7, Avg Loss : 3.8725
Epoch : 8, Avg Loss : 3.8824
Epoch : 9, Avg Loss : 3.8625
Epoch : 10, Avg Loss : 3.8634
Epoch : 11, Avg Loss : 3.8292
Epoch : 12, Avg Loss : 3.8561
Epoch : 13, Avg Loss : 3.8516
Epoch : 14, Avg Loss : 3.8459
Epoch : 15, Avg Loss : 3.8088
Epoch : 16, Avg Loss : 3.8535
Epoch : 17, Avg Loss : 3.8388
Epoch : 18, Avg Loss : 3.8427
Epoch : 19, Avg Loss : 3.8704
Epoch : 20, Avg Loss : 3.8678
Epoch : 21, Avg Loss : 3.8904
Epoch : 22, Avg Loss : 3.8364
Epoch : 23, Avg Loss : 3.8120
Epoch : 24, Avg Loss : 3.7405
Epoch : 25, Avg Loss : 3.8671
Epoch : 26, Avg Loss : 3.8905
Epoch : 27, Avg Loss : 3.8746
Epoch : 28, Avg Loss : 3.8577
Epoch : 29, Avg Loss : 3.8685
FG 학습 완료. 이제 FG의 output을 실제 dataset의 label과 연결.
Epoch : 0, Avg Loss : 1.9977
Epoch : 1, Avg Loss : 1.9155
Epoch : 2, Avg Loss : 1.8995
Epoch : 3, Avg Loss : 1.8917
Epoch : 4, Avg Loss : 1.8860
Epoch : 5, Avg Loss : 1.8826
Epoch : 6, Avg Loss : 1.8801
Epoch : 7, Avg Loss : 1.8768
Epoch : 8, Avg Loss : 1.8747
Epoch : 9, Avg Loss : 1.8740
Epoch : 10, Avg Loss : 1.8723
Epoch : 11, Avg Loss : 1.8711
Epoch : 12, Avg Loss : 1.8703
Epoch : 13, Avg Loss : 1.8690
Epoch : 14, Avg Loss : 1.8686
Epoch : 15, Avg Loss : 1.8672
Epoch : 16, Avg Loss : 1.8667
Epoch : 17, Avg Loss : 1.8660
Epoch : 18, Avg Loss : 1.8647
Epoch : 19, Avg Loss : 1.8644
Epoch : 20, Avg Loss : 1.8636
Epoch : 21, Avg Loss : 1.8624
Epoch : 22, Avg Loss : 1.8619
Epoch : 23, Avg Loss : 1.8614
Epoch : 24, Avg Loss : 1.8603
Epoch : 25, Avg Loss : 1.8599
Epoch : 26, Avg Loss : 1.8599
Epoch : 27, Avg Loss : 1.8598
Epoch : 28, Avg Loss : 1.8582
Epoch : 29, Avg Loss : 1.8585
실제 test
총 개수 : 10000
 맞춘 개수 : 3099
 정확도: 30
 찍었을 때의 정확도 : 10

종료 코드 0(으)로 완료된 프로세스
