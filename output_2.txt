특이사항: Horizontal, Vertical Flip 추가


C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main.py
Files already downloaded and verified
Files already downloaded and verified
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
Sequential                                    [64, 32]                  --
├─Resnet: 1-1                                 [64, 32]                  --
│    └─Conv2d: 2-1                            [64, 64, 16, 16]          9,408
│    └─Sequential: 2-2                        [64, 64, 8, 8]            --
│    │    └─MaxPool2d: 3-1                    [64, 64, 8, 8]            --
│    │    └─ResnetBlock: 3-2                  [64, 64, 8, 8]            73,984
│    │    └─ResnetBlock: 3-3                  [64, 64, 8, 8]            73,984
│    └─Sequential: 2-3                        [64, 128, 8, 8]           --
│    │    └─ResnetBlock: 3-4                  [64, 128, 8, 8]           230,144
│    │    └─ResnetBlock: 3-5                  [64, 128, 8, 8]           295,424
│    │    └─ResnetBlock: 3-6                  [64, 128, 8, 8]           295,424
│    └─Sequential: 2-4                        [64, 256, 8, 8]           --
│    │    └─ResnetBlock: 3-7                  [64, 256, 8, 8]           919,040
│    │    └─ResnetBlock: 3-8                  [64, 256, 8, 8]           1,180,672
│    │    └─ResnetBlock: 3-9                  [64, 256, 8, 8]           1,180,672
│    └─Sequential: 2-5                        [64, 512, 8, 8]           --
│    │    └─ResnetBlock: 3-10                 [64, 512, 8, 8]           3,673,088
│    │    └─ResnetBlock: 3-11                 [64, 512, 8, 8]           4,720,640
│    │    └─ResnetBlock: 3-12                 [64, 512, 8, 8]           4,720,640
│    └─AdaptiveAvgPool2d: 2-6                 [64, 512, 1, 1]           --
│    └─Sequential: 2-7                        [64, 32]                  --
│    │    └─Flatten: 3-13                     [64, 512]                 --
│    │    └─Linear: 3-14                      [64, 32]                  16,416
├─SimpleMLP: 1-2                              [64, 32]                  --
│    └─Linear: 2-8                            [64, 32]                  1,056
│    └─ReLU: 2-9                              [64, 32]                  --
===============================================================================================
Total params: 17,390,592
Trainable params: 17,390,592
Non-trainable params: 0
Total mult-adds (G): 71.22
===============================================================================================
Input size (MB): 0.79
Forward/backward pass size (MB): 436.24
Params size (MB): 69.56
Estimated Total Size (MB): 506.59
===============================================================================================
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch : 0, Avg Loss : 4.0770
Epoch : 1, Avg Loss : 3.9213
Epoch : 2, Avg Loss : 3.8728
Epoch : 3, Avg Loss : 3.8477
Epoch : 4, Avg Loss : 3.8263
Epoch : 5, Avg Loss : 3.8152
Epoch : 6, Avg Loss : 3.8114
Epoch : 7, Avg Loss : 3.7999
Epoch : 8, Avg Loss : 3.7884
Epoch : 9, Avg Loss : 3.7884
FG 학습 완료. 이제 FG의 output을 실제 dataset의 label과 연결.
Epoch : 0, Avg Loss : 2.1537
Epoch : 1, Avg Loss : 2.0117
Epoch : 2, Avg Loss : 1.9985
Epoch : 3, Avg Loss : 1.9920
Epoch : 4, Avg Loss : 1.9881
Epoch : 5, Avg Loss : 1.9832
Epoch : 6, Avg Loss : 1.9849
Epoch : 7, Avg Loss : 1.9819
Epoch : 8, Avg Loss : 1.9815
Epoch : 9, Avg Loss : 1.9799
Epoch : 10, Avg Loss : 1.9811
Epoch : 11, Avg Loss : 1.9807
Epoch : 12, Avg Loss : 1.9784
Epoch : 13, Avg Loss : 1.9773
Epoch : 14, Avg Loss : 1.9805
Epoch : 15, Avg Loss : 1.9768
Epoch : 16, Avg Loss : 1.9773
Epoch : 17, Avg Loss : 1.9766
Epoch : 18, Avg Loss : 1.9753
Epoch : 19, Avg Loss : 1.9760
실제 test
총 개수 : 50000
 맞춘 개수 : 13191
 정확도: 26
 찍었을 때의 정확도 : 10