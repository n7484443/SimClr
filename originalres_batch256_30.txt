C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main_with_originalres.py
Files already downloaded and verified
Files already downloaded and verified
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
Sequential                                    [256, 10]                 --
├─ResNet: 1-1                                 [256, 32]                 --
│    └─Conv2d: 2-1                            [256, 64, 16, 16]         9,408
│    └─BatchNorm2d: 2-2                       [256, 64, 16, 16]         128
│    └─ReLU: 2-3                              [256, 64, 16, 16]         --
│    └─MaxPool2d: 2-4                         [256, 64, 8, 8]           --
│    └─Sequential: 2-5                        [256, 64, 8, 8]           --
│    │    └─BasicBlock: 3-1                   [256, 64, 8, 8]           73,984
│    │    └─BasicBlock: 3-2                   [256, 64, 8, 8]           73,984
│    └─Sequential: 2-6                        [256, 128, 4, 4]          --
│    │    └─BasicBlock: 3-3                   [256, 128, 4, 4]          230,144
│    │    └─BasicBlock: 3-4                   [256, 128, 4, 4]          295,424
│    └─Sequential: 2-7                        [256, 256, 2, 2]          --
│    │    └─BasicBlock: 3-5                   [256, 256, 2, 2]          919,040
│    │    └─BasicBlock: 3-6                   [256, 256, 2, 2]          1,180,672
│    └─Sequential: 2-8                        [256, 512, 1, 1]          --
│    │    └─BasicBlock: 3-7                   [256, 512, 1, 1]          3,673,088
│    │    └─BasicBlock: 3-8                   [256, 512, 1, 1]          4,720,640
│    └─AdaptiveAvgPool2d: 2-9                 [256, 512, 1, 1]          --
│    └─Linear: 2-10                           [256, 32]                 16,416
├─SimpleMLP: 1-2                              [256, 10]                 --
│    └─Linear: 2-11                           [256, 10]                 330
│    └─ReLU: 2-12                             [256, 10]                 --
│    └─Linear: 2-13                           [256, 10]                 110
===============================================================================================
Total params: 11,193,368
Trainable params: 11,193,368
Non-trainable params: 0
Total mult-adds (G): 9.48
===============================================================================================
Input size (MB): 3.15
Forward/backward pass size (MB): 207.72
Params size (MB): 44.77
Estimated Total Size (MB): 255.64
===============================================================================================
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch : 0, Avg Loss : 3.6412 lr: 0.00030000
Epoch : 1, Avg Loss : 1.6328 lr: 0.00028500
Epoch : 2, Avg Loss : 1.4439 lr: 0.00027075
Epoch : 3, Avg Loss : 1.2864 lr: 0.00025721
Epoch : 4, Avg Loss : 1.1497 lr: 0.00024435
Epoch : 5, Avg Loss : 1.0890 lr: 0.00023213
Epoch : 6, Avg Loss : 1.0791 lr: 0.00022053
Epoch : 7, Avg Loss : 1.0624 lr: 0.00020950
Epoch : 8, Avg Loss : 0.9697 lr: 0.00019903
Epoch : 9, Avg Loss : 0.9097 lr: 0.00018907
Epoch : 10, Avg Loss : 0.8977 lr: 0.00017962
Epoch : 11, Avg Loss : 0.8379 lr: 0.00017064
Epoch : 12, Avg Loss : 0.8141 lr: 0.00016211
Epoch : 13, Avg Loss : 0.7770 lr: 0.00015400
Epoch : 14, Avg Loss : 0.7762 lr: 0.00014630
Epoch : 15, Avg Loss : 0.7806 lr: 0.00013899
Epoch : 16, Avg Loss : 0.7119 lr: 0.00013204
Epoch : 17, Avg Loss : 0.7021 lr: 0.00012544
Epoch : 18, Avg Loss : 0.6964 lr: 0.00011916
Epoch : 19, Avg Loss : 0.6454 lr: 0.00011321
Epoch : 20, Avg Loss : 0.6785 lr: 0.00010755
Epoch : 21, Avg Loss : 0.6186 lr: 0.00010217
Epoch : 22, Avg Loss : 0.6210 lr: 0.00009706
Epoch : 23, Avg Loss : 0.6257 lr: 0.00009221
Epoch : 24, Avg Loss : 0.6191 lr: 0.00008760
Epoch : 25, Avg Loss : 0.6212 lr: 0.00008322
Epoch : 26, Avg Loss : 0.5847 lr: 0.00007906
Epoch : 27, Avg Loss : 0.5867 lr: 0.00007510
Epoch : 28, Avg Loss : 0.5975 lr: 0.00007135
Epoch : 29, Avg Loss : 0.5846 lr: 0.00006778
FG 학습 완료. 이제 F의 output을 실제 dataset의 label과 연결.
Epoch : 0, Avg Loss : 2.3472 lr: 0.00030000
Epoch : 1, Avg Loss : 2.1714 lr: 0.00028500
Epoch : 2, Avg Loss : 2.0907 lr: 0.00027075
Epoch : 3, Avg Loss : 2.0214 lr: 0.00025721
Epoch : 4, Avg Loss : 1.9417 lr: 0.00024435
Epoch : 5, Avg Loss : 1.8728 lr: 0.00023213
Epoch : 6, Avg Loss : 1.8267 lr: 0.00022053
Epoch : 7, Avg Loss : 1.7883 lr: 0.00020950
Epoch : 8, Avg Loss : 1.7599 lr: 0.00019903
Epoch : 9, Avg Loss : 1.7355 lr: 0.00018907
Epoch : 10, Avg Loss : 1.7171 lr: 0.00017962
Epoch : 11, Avg Loss : 1.7018 lr: 0.00017064
Epoch : 12, Avg Loss : 1.6852 lr: 0.00016211
Epoch : 13, Avg Loss : 1.6709 lr: 0.00015400
Epoch : 14, Avg Loss : 1.6613 lr: 0.00014630
Epoch : 15, Avg Loss : 1.6510 lr: 0.00013899
Epoch : 16, Avg Loss : 1.6455 lr: 0.00013204
Epoch : 17, Avg Loss : 1.6349 lr: 0.00012544
Epoch : 18, Avg Loss : 1.6283 lr: 0.00011916
Epoch : 19, Avg Loss : 1.6221 lr: 0.00011321
Epoch : 20, Avg Loss : 1.6158 lr: 0.00010755
Epoch : 21, Avg Loss : 1.6115 lr: 0.00010217
Epoch : 22, Avg Loss : 1.6076 lr: 0.00009706
Epoch : 23, Avg Loss : 1.6026 lr: 0.00009221
Epoch : 24, Avg Loss : 1.6001 lr: 0.00008760
Epoch : 25, Avg Loss : 1.5948 lr: 0.00008322
Epoch : 26, Avg Loss : 1.5913 lr: 0.00007906
Epoch : 27, Avg Loss : 1.5892 lr: 0.00007510
Epoch : 28, Avg Loss : 1.5878 lr: 0.00007135
Epoch : 29, Avg Loss : 1.5851 lr: 0.00006778
실제 test
총 개수 : 10000
 맞춘 개수 : 4499
 정확도: 44
 찍었을 때의 정확도 : 10

종료 코드 0(으)로 완료된 프로세스
