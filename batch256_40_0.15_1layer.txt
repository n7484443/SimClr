C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main_with_originalres.py
Files already downloaded and verified
Files already downloaded and verified
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
Sequential                                    [256, 32]                 --
├─ResNet: 1-1                                 [256, 32]                 --
│    └─Conv2d: 2-1                            [256, 64, 16, 16]         9,408
│    └─BatchNorm2d: 2-2                       [256, 64, 16, 16]         128
│    └─ReLU: 2-3                              [256, 64, 16, 16]         --
│    └─MaxPool2d: 2-4                         [256, 64, 8, 8]           --
│    └─Sequential: 2-5                        [256, 64, 8, 8]           --
│    │    └─BasicBlock: 3-1                   [256, 64, 8, 8]           73,984
│    │    └─BasicBlock: 3-2                   [256, 64, 8, 8]           73,984
│    └─Sequential: 2-6                        [256, 128, 4, 4]          --
│    │    └─BasicBlock: 3-3                   [256, 128, 4, 4]          230,144
│    │    └─BasicBlock: 3-4                   [256, 128, 4, 4]          295,424
│    └─Sequential: 2-7                        [256, 256, 2, 2]          --
│    │    └─BasicBlock: 3-5                   [256, 256, 2, 2]          919,040
│    │    └─BasicBlock: 3-6                   [256, 256, 2, 2]          1,180,672
│    └─Sequential: 2-8                        [256, 512, 1, 1]          --
│    │    └─BasicBlock: 3-7                   [256, 512, 1, 1]          3,673,088
│    │    └─BasicBlock: 3-8                   [256, 512, 1, 1]          4,720,640
│    └─AdaptiveAvgPool2d: 2-9                 [256, 512, 1, 1]          --
│    └─Linear: 2-10                           [256, 32]                 16,416
├─SimpleMLP: 1-2                              [256, 32]                 --
│    └─Linear: 2-11                           [256, 32]                 1,056
│    └─ReLU: 2-12                             [256, 32]                 --
│    └─Linear: 2-13                           [256, 32]                 1,056
===============================================================================================
Total params: 11,195,040
Trainable params: 11,195,040
Non-trainable params: 0
Total mult-adds (G): 9.48
===============================================================================================
Input size (MB): 3.15
Forward/backward pass size (MB): 207.81
Params size (MB): 44.78
Estimated Total Size (MB): 255.74
===============================================================================================
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch : 0, Avg Loss : 3.1566 lr: 0.00030000
Epoch : 1, Avg Loss : 2.1302 lr: 0.00028500
Epoch : 2, Avg Loss : 1.8556 lr: 0.00027075
Epoch : 3, Avg Loss : 1.7589 lr: 0.00025721
Epoch : 4, Avg Loss : 1.6459 lr: 0.00024435
Epoch : 5, Avg Loss : 1.6162 lr: 0.00023213
Epoch : 6, Avg Loss : 1.5369 lr: 0.00022053
Epoch : 7, Avg Loss : 1.5465 lr: 0.00020950
Epoch : 8, Avg Loss : 1.4835 lr: 0.00019903
Epoch : 9, Avg Loss : 1.4397 lr: 0.00018907
Epoch : 10, Avg Loss : 1.4739 lr: 0.00017962
Epoch : 11, Avg Loss : 1.4013 lr: 0.00017064
Epoch : 12, Avg Loss : 1.4180 lr: 0.00016211
Epoch : 13, Avg Loss : 1.3790 lr: 0.00015400
Epoch : 14, Avg Loss : 1.3732 lr: 0.00014630
Epoch : 15, Avg Loss : 1.3716 lr: 0.00013899
Epoch : 16, Avg Loss : 1.3308 lr: 0.00013204
Epoch : 17, Avg Loss : 1.3533 lr: 0.00012544
Epoch : 18, Avg Loss : 1.3075 lr: 0.00011916
Epoch : 19, Avg Loss : 1.3150 lr: 0.00011321
Epoch : 20, Avg Loss : 1.2737 lr: 0.00010755
Epoch : 21, Avg Loss : 1.3071 lr: 0.00010217
Epoch : 22, Avg Loss : 1.2903 lr: 0.00009706
Epoch : 23, Avg Loss : 1.2688 lr: 0.00009221
Epoch : 24, Avg Loss : 1.2591 lr: 0.00008760
Epoch : 25, Avg Loss : 1.2539 lr: 0.00008322
Epoch : 26, Avg Loss : 1.2290 lr: 0.00007906
Epoch : 27, Avg Loss : 1.2367 lr: 0.00007510
Epoch : 28, Avg Loss : 1.2152 lr: 0.00007135
Epoch : 29, Avg Loss : 1.2221 lr: 0.00006778
Epoch : 30, Avg Loss : 1.2253 lr: 0.00006439
Epoch : 31, Avg Loss : 1.2335 lr: 0.00006117
Epoch : 32, Avg Loss : 1.2456 lr: 0.00005811
Epoch : 33, Avg Loss : 1.2246 lr: 0.00005521
Epoch : 34, Avg Loss : 1.2187 lr: 0.00005245
Epoch : 35, Avg Loss : 1.1964 lr: 0.00004983
Epoch : 36, Avg Loss : 1.1733 lr: 0.00004733
Epoch : 37, Avg Loss : 1.2016 lr: 0.00004497
Epoch : 38, Avg Loss : 1.1762 lr: 0.00004272
Epoch : 39, Avg Loss : 1.1858 lr: 0.00004058
FG 학습 완료. 이제 F의 output을 실제 dataset의 label과 연결.
Epoch : 0, Avg Loss : 2.2803 lr: 0.00030000
Epoch : 1, Avg Loss : 2.1967 lr: 0.00028500
Epoch : 2, Avg Loss : 2.1256 lr: 0.00027075
Epoch : 3, Avg Loss : 2.0649 lr: 0.00025721
Epoch : 4, Avg Loss : 2.0126 lr: 0.00024435
Epoch : 5, Avg Loss : 1.9669 lr: 0.00023213
Epoch : 6, Avg Loss : 1.9260 lr: 0.00022053
Epoch : 7, Avg Loss : 1.8908 lr: 0.00020950
Epoch : 8, Avg Loss : 1.8596 lr: 0.00019903
Epoch : 9, Avg Loss : 1.8334 lr: 0.00018907
Epoch : 10, Avg Loss : 1.8083 lr: 0.00017962
Epoch : 11, Avg Loss : 1.7861 lr: 0.00017064
Epoch : 12, Avg Loss : 1.7664 lr: 0.00016211
Epoch : 13, Avg Loss : 1.7486 lr: 0.00015400
Epoch : 14, Avg Loss : 1.7323 lr: 0.00014630
Epoch : 15, Avg Loss : 1.7195 lr: 0.00013899
Epoch : 16, Avg Loss : 1.7055 lr: 0.00013204
Epoch : 17, Avg Loss : 1.6937 lr: 0.00012544
Epoch : 18, Avg Loss : 1.6831 lr: 0.00011916
Epoch : 19, Avg Loss : 1.6736 lr: 0.00011321
Epoch : 20, Avg Loss : 1.6640 lr: 0.00010755
Epoch : 21, Avg Loss : 1.6558 lr: 0.00010217
Epoch : 22, Avg Loss : 1.6472 lr: 0.00009706
Epoch : 23, Avg Loss : 1.6409 lr: 0.00009221
Epoch : 24, Avg Loss : 1.6346 lr: 0.00008760
Epoch : 25, Avg Loss : 1.6287 lr: 0.00008322
Epoch : 26, Avg Loss : 1.6233 lr: 0.00007906
Epoch : 27, Avg Loss : 1.6186 lr: 0.00007510
Epoch : 28, Avg Loss : 1.6136 lr: 0.00007135
Epoch : 29, Avg Loss : 1.6091 lr: 0.00006778
Epoch : 30, Avg Loss : 1.6045 lr: 0.00006439
Epoch : 31, Avg Loss : 1.6006 lr: 0.00006117
Epoch : 32, Avg Loss : 1.5976 lr: 0.00005811
Epoch : 33, Avg Loss : 1.5927 lr: 0.00005521
Epoch : 34, Avg Loss : 1.5904 lr: 0.00005245
Epoch : 35, Avg Loss : 1.5881 lr: 0.00004983
Epoch : 36, Avg Loss : 1.5836 lr: 0.00004733
Epoch : 37, Avg Loss : 1.5825 lr: 0.00004497
Epoch : 38, Avg Loss : 1.5797 lr: 0.00004272
Epoch : 39, Avg Loss : 1.5773 lr: 0.00004058
실제 test
총 개수 : 10000
 맞춘 개수 : 5106
 정확도: 51.06
 찍었을 때의 정확도 : 10

종료 코드 0(으)로 완료된 프로세스
