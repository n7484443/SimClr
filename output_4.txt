Resnet을 논문의 32 사이즈로 설정

C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main.py
Files already downloaded and verified
Files already downloaded and verified
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
Sequential                                    [512, 32]                 --
├─Resnet: 1-1                                 [512, 32]                 --
│    └─Sequential: 2-1                        [512, 16, 32, 32]         --
│    │    └─Conv2d: 3-1                       [512, 16, 32, 32]         432
│    │    └─BatchNorm2d: 3-2                  [512, 16, 32, 32]         32
│    │    └─ReLU: 3-3                         [512, 16, 32, 32]         --
│    └─Sequential: 2-2                        [512, 32, 16, 16]         --
│    │    └─ResnetBlock: 3-4                  [512, 32, 16, 16]         14,528
│    │    └─ResnetBlock: 3-5                  [512, 32, 16, 16]         18,560
│    │    └─ResnetBlock: 3-6                  [512, 32, 16, 16]         18,560
│    │    └─ResnetBlock: 3-7                  [512, 32, 16, 16]         18,560
│    │    └─ResnetBlock: 3-8                  [512, 32, 16, 16]         18,560
│    └─Sequential: 2-3                        [512, 64, 8, 8]           --
│    │    └─ResnetBlock: 3-9                  [512, 64, 8, 8]           57,728
│    │    └─ResnetBlock: 3-10                 [512, 64, 8, 8]           73,984
│    │    └─ResnetBlock: 3-11                 [512, 64, 8, 8]           73,984
│    │    └─ResnetBlock: 3-12                 [512, 64, 8, 8]           73,984
│    │    └─ResnetBlock: 3-13                 [512, 64, 8, 8]           73,984
│    └─Sequential: 2-4                        [512, 128, 4, 4]          --
│    │    └─ResnetBlock: 3-14                 [512, 128, 4, 4]          230,144
│    │    └─ResnetBlock: 3-15                 [512, 128, 4, 4]          295,424
│    │    └─ResnetBlock: 3-16                 [512, 128, 4, 4]          295,424
│    │    └─ResnetBlock: 3-17                 [512, 128, 4, 4]          295,424
│    │    └─ResnetBlock: 3-18                 [512, 128, 4, 4]          295,424
│    └─AdaptiveAvgPool2d: 2-5                 [512, 128, 1, 1]          --
│    └─Sequential: 2-6                        [512, 32]                 --
│    │    └─Flatten: 3-19                     [512, 128]                --
│    │    └─Linear: 3-20                      [512, 32]                 4,128
├─SimpleMLP: 1-2                              [512, 32]                 --
│    └─Linear: 2-7                            [512, 32]                 1,056
│    └─ReLU: 2-8                              [512, 32]                 --
===============================================================================================
Total params: 1,859,920
Trainable params: 1,859,920
Non-trainable params: 0
Total mult-adds (G): 34.86
===============================================================================================
Input size (MB): 6.29
Forward/backward pass size (MB): 1426.33
Params size (MB): 7.44
Estimated Total Size (MB): 1440.06
===============================================================================================
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch : 0, Avg Loss : 6.8181
Epoch : 1, Avg Loss : 6.4127
Epoch : 2, Avg Loss : 6.3427
Epoch : 3, Avg Loss : 6.2674
Epoch : 4, Avg Loss : 6.1669
Epoch : 5, Avg Loss : 6.1101
Epoch : 6, Avg Loss : 6.0734
Epoch : 7, Avg Loss : 6.0812
Epoch : 8, Avg Loss : 6.0370
Epoch : 9, Avg Loss : 6.0294
Epoch : 10, Avg Loss : 6.0109
Epoch : 11, Avg Loss : 5.9626
Epoch : 12, Avg Loss : 5.9285
Epoch : 13, Avg Loss : 5.8747
Epoch : 14, Avg Loss : 5.9500
Epoch : 15, Avg Loss : 5.9421
Epoch : 16, Avg Loss : 5.9312
Epoch : 17, Avg Loss : 5.8509
Epoch : 18, Avg Loss : 5.8470
Epoch : 19, Avg Loss : 5.8293
Epoch : 20, Avg Loss : 5.7944
Epoch : 21, Avg Loss : 5.7774
Epoch : 22, Avg Loss : 5.8367
Epoch : 23, Avg Loss : 5.7963
Epoch : 24, Avg Loss : 5.7787
Epoch : 25, Avg Loss : 5.7296
Epoch : 26, Avg Loss : 5.7166
Epoch : 27, Avg Loss : 5.7812
Epoch : 28, Avg Loss : 5.7029
Epoch : 29, Avg Loss : 5.7164
Epoch : 30, Avg Loss : 5.7208
Epoch : 31, Avg Loss : 5.7071
Epoch : 32, Avg Loss : 5.6401
Epoch : 33, Avg Loss : 5.6601
Epoch : 34, Avg Loss : 5.6803
Epoch : 35, Avg Loss : 5.6386
Epoch : 36, Avg Loss : 5.6299
Epoch : 37, Avg Loss : 5.6451
Epoch : 38, Avg Loss : 5.6533
Epoch : 39, Avg Loss : 5.5925
FG 학습 완료. 이제 FG의 output을 실제 dataset의 label과 연결.
Epoch : 0, Avg Loss : 2.1961
Epoch : 1, Avg Loss : 1.9539
Epoch : 2, Avg Loss : 1.9261
Epoch : 3, Avg Loss : 1.9171
Epoch : 4, Avg Loss : 1.9118
Epoch : 5, Avg Loss : 1.9080
Epoch : 6, Avg Loss : 1.9058
Epoch : 7, Avg Loss : 1.9041
Epoch : 8, Avg Loss : 1.9036
Epoch : 9, Avg Loss : 1.9030
Epoch : 10, Avg Loss : 1.9028
Epoch : 11, Avg Loss : 1.9020
Epoch : 12, Avg Loss : 1.9023
Epoch : 13, Avg Loss : 1.9020
Epoch : 14, Avg Loss : 1.9019
Epoch : 15, Avg Loss : 1.9016
Epoch : 16, Avg Loss : 1.9016
Epoch : 17, Avg Loss : 1.9015
Epoch : 18, Avg Loss : 1.9015
Epoch : 19, Avg Loss : 1.9016
Epoch : 20, Avg Loss : 1.9013
Epoch : 21, Avg Loss : 1.9010
Epoch : 22, Avg Loss : 1.9014
Epoch : 23, Avg Loss : 1.9009
Epoch : 24, Avg Loss : 1.9008
Epoch : 25, Avg Loss : 1.9008
Epoch : 26, Avg Loss : 1.9010
Epoch : 27, Avg Loss : 1.9011
Epoch : 28, Avg Loss : 1.9007
Epoch : 29, Avg Loss : 1.9008
Epoch : 30, Avg Loss : 1.9006
Epoch : 31, Avg Loss : 1.9009
Epoch : 32, Avg Loss : 1.9006
Epoch : 33, Avg Loss : 1.9002
Epoch : 34, Avg Loss : 1.9008
Epoch : 35, Avg Loss : 1.9005
Epoch : 36, Avg Loss : 1.9003
Epoch : 37, Avg Loss : 1.9007
Epoch : 38, Avg Loss : 1.9002
Epoch : 39, Avg Loss : 1.9003
실제 test
총 개수 : 10000
 맞춘 개수 : 2969
 정확도: 29
 찍었을 때의 정확도 : 10

종료 코드 0(으)로 완료된 프로세스
