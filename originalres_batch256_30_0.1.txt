C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main_with_originalres.py
Files already downloaded and verified
Files already downloaded and verified
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
Sequential                                    [256, 10]                 --
├─ResNet: 1-1                                 [256, 32]                 --
│    └─Conv2d: 2-1                            [256, 64, 16, 16]         9,408
│    └─BatchNorm2d: 2-2                       [256, 64, 16, 16]         128
│    └─ReLU: 2-3                              [256, 64, 16, 16]         --
│    └─MaxPool2d: 2-4                         [256, 64, 8, 8]           --
│    └─Sequential: 2-5                        [256, 64, 8, 8]           --
│    │    └─BasicBlock: 3-1                   [256, 64, 8, 8]           73,984
│    │    └─BasicBlock: 3-2                   [256, 64, 8, 8]           73,984
│    └─Sequential: 2-6                        [256, 128, 4, 4]          --
│    │    └─BasicBlock: 3-3                   [256, 128, 4, 4]          230,144
│    │    └─BasicBlock: 3-4                   [256, 128, 4, 4]          295,424
│    └─Sequential: 2-7                        [256, 256, 2, 2]          --
│    │    └─BasicBlock: 3-5                   [256, 256, 2, 2]          919,040
│    │    └─BasicBlock: 3-6                   [256, 256, 2, 2]          1,180,672
│    └─Sequential: 2-8                        [256, 512, 1, 1]          --
│    │    └─BasicBlock: 3-7                   [256, 512, 1, 1]          3,673,088
│    │    └─BasicBlock: 3-8                   [256, 512, 1, 1]          4,720,640
│    └─AdaptiveAvgPool2d: 2-9                 [256, 512, 1, 1]          --
│    └─Linear: 2-10                           [256, 32]                 16,416
├─SimpleMLP: 1-2                              [256, 10]                 --
│    └─Linear: 2-11                           [256, 10]                 330
│    └─ReLU: 2-12                             [256, 10]                 --
│    └─Linear: 2-13                           [256, 10]                 110
===============================================================================================
Total params: 11,193,368
Trainable params: 11,193,368
Non-trainable params: 0
Total mult-adds (G): 9.48
===============================================================================================
Input size (MB): 3.15
Forward/backward pass size (MB): 207.72
Params size (MB): 44.77
Estimated Total Size (MB): 255.64
===============================================================================================
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch : 0, Avg Loss : 3.8707 lr: 0.00030000
Epoch : 1, Avg Loss : 2.0154 lr: 0.00028500
Epoch : 2, Avg Loss : 1.7172 lr: 0.00027075
Epoch : 3, Avg Loss : 1.5004 lr: 0.00025721
Epoch : 4, Avg Loss : 1.4672 lr: 0.00024435
Epoch : 5, Avg Loss : 1.3909 lr: 0.00023213
Epoch : 6, Avg Loss : 1.3698 lr: 0.00022053
Epoch : 7, Avg Loss : 1.3626 lr: 0.00020950
Epoch : 8, Avg Loss : 1.3417 lr: 0.00019903
Epoch : 9, Avg Loss : 1.2882 lr: 0.00018907
Epoch : 10, Avg Loss : 1.2885 lr: 0.00017962
Epoch : 11, Avg Loss : 1.2554 lr: 0.00017064
Epoch : 12, Avg Loss : 1.2640 lr: 0.00016211
Epoch : 13, Avg Loss : 1.2812 lr: 0.00015400
Epoch : 14, Avg Loss : 1.2303 lr: 0.00014630
Epoch : 15, Avg Loss : 1.2124 lr: 0.00013899
Epoch : 16, Avg Loss : 1.1761 lr: 0.00013204
Epoch : 17, Avg Loss : 1.1401 lr: 0.00012544
Epoch : 18, Avg Loss : 1.1294 lr: 0.00011916
Epoch : 19, Avg Loss : 1.1034 lr: 0.00011321
Epoch : 20, Avg Loss : 1.0725 lr: 0.00010755
Epoch : 21, Avg Loss : 1.0939 lr: 0.00010217
Epoch : 22, Avg Loss : 1.0861 lr: 0.00009706
Epoch : 23, Avg Loss : 1.0597 lr: 0.00009221
Epoch : 24, Avg Loss : 1.0635 lr: 0.00008760
Epoch : 25, Avg Loss : 1.0595 lr: 0.00008322
Epoch : 26, Avg Loss : 1.0533 lr: 0.00007906
Epoch : 27, Avg Loss : 1.0145 lr: 0.00007510
Epoch : 28, Avg Loss : 0.9942 lr: 0.00007135
Epoch : 29, Avg Loss : 0.9922 lr: 0.00006778
FG 학습 완료. 이제 F의 output을 실제 dataset의 label과 연결.
Epoch : 0, Avg Loss : 2.3149 lr: 0.00030000
Epoch : 1, Avg Loss : 2.1233 lr: 0.00028500
Epoch : 2, Avg Loss : 2.0103 lr: 0.00027075
Epoch : 3, Avg Loss : 1.9250 lr: 0.00025721
Epoch : 4, Avg Loss : 1.8537 lr: 0.00024435
Epoch : 5, Avg Loss : 1.7980 lr: 0.00023213
Epoch : 6, Avg Loss : 1.7556 lr: 0.00022053
Epoch : 7, Avg Loss : 1.7223 lr: 0.00020950
Epoch : 8, Avg Loss : 1.6915 lr: 0.00019903
Epoch : 9, Avg Loss : 1.6680 lr: 0.00018907
Epoch : 10, Avg Loss : 1.6462 lr: 0.00017962
Epoch : 11, Avg Loss : 1.6302 lr: 0.00017064
Epoch : 12, Avg Loss : 1.6184 lr: 0.00016211
Epoch : 13, Avg Loss : 1.6087 lr: 0.00015400
Epoch : 14, Avg Loss : 1.6004 lr: 0.00014630
Epoch : 15, Avg Loss : 1.5923 lr: 0.00013899
Epoch : 16, Avg Loss : 1.5868 lr: 0.00013204
Epoch : 17, Avg Loss : 1.5781 lr: 0.00012544
Epoch : 18, Avg Loss : 1.5786 lr: 0.00011916
Epoch : 19, Avg Loss : 1.5740 lr: 0.00011321
Epoch : 20, Avg Loss : 1.5707 lr: 0.00010755
Epoch : 21, Avg Loss : 1.5665 lr: 0.00010217
Epoch : 22, Avg Loss : 1.5615 lr: 0.00009706
Epoch : 23, Avg Loss : 1.5595 lr: 0.00009221
Epoch : 24, Avg Loss : 1.5572 lr: 0.00008760
Epoch : 25, Avg Loss : 1.5544 lr: 0.00008322
Epoch : 26, Avg Loss : 1.5525 lr: 0.00007906
Epoch : 27, Avg Loss : 1.5502 lr: 0.00007510
Epoch : 28, Avg Loss : 1.5490 lr: 0.00007135
Epoch : 29, Avg Loss : 1.5481 lr: 0.00006778
실제 test
총 개수 : 10000
 맞춘 개수 : 4636
 정확도: 46
 찍었을 때의 정확도 : 10

종료 코드 0(으)로 완료된 프로세스
