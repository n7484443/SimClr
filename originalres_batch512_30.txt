C:\Users\kimJuhwan\anaconda3\envs\pytorch\python.exe E:\github\simclrExe\main_with_originalres.py
Files already downloaded and verified
Files already downloaded and verified
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
Sequential                                    [512, 10]                 --
├─ResNet: 1-1                                 [512, 32]                 --
│    └─Conv2d: 2-1                            [512, 64, 16, 16]         9,408
│    └─BatchNorm2d: 2-2                       [512, 64, 16, 16]         128
│    └─ReLU: 2-3                              [512, 64, 16, 16]         --
│    └─MaxPool2d: 2-4                         [512, 64, 8, 8]           --
│    └─Sequential: 2-5                        [512, 64, 8, 8]           --
│    │    └─BasicBlock: 3-1                   [512, 64, 8, 8]           73,984
│    │    └─BasicBlock: 3-2                   [512, 64, 8, 8]           73,984
│    └─Sequential: 2-6                        [512, 128, 4, 4]          --
│    │    └─BasicBlock: 3-3                   [512, 128, 4, 4]          230,144
│    │    └─BasicBlock: 3-4                   [512, 128, 4, 4]          295,424
│    └─Sequential: 2-7                        [512, 256, 2, 2]          --
│    │    └─BasicBlock: 3-5                   [512, 256, 2, 2]          919,040
│    │    └─BasicBlock: 3-6                   [512, 256, 2, 2]          1,180,672
│    └─Sequential: 2-8                        [512, 512, 1, 1]          --
│    │    └─BasicBlock: 3-7                   [512, 512, 1, 1]          3,673,088
│    │    └─BasicBlock: 3-8                   [512, 512, 1, 1]          4,720,640
│    └─AdaptiveAvgPool2d: 2-9                 [512, 512, 1, 1]          --
│    └─Linear: 2-10                           [512, 32]                 16,416
├─SimpleMLP: 1-2                              [512, 10]                 --
│    └─Linear: 2-11                           [512, 10]                 330
│    └─ReLU: 2-12                             [512, 10]                 --
│    └─Linear: 2-13                           [512, 10]                 110
===============================================================================================
Total params: 11,193,368
Trainable params: 11,193,368
Non-trainable params: 0
Total mult-adds (G): 18.96
===============================================================================================
Input size (MB): 6.29
Forward/backward pass size (MB): 415.45
Params size (MB): 44.77
Estimated Total Size (MB): 466.51
===============================================================================================
C:\Users\kimJuhwan\anaconda3\envs\pytorch\lib\site-packages\torchvision\transforms\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Epoch : 0, Avg Loss : 5.0669 lr: 0.00030000
Epoch : 1, Avg Loss : 3.2425 lr: 0.00028500
Epoch : 2, Avg Loss : 2.3729 lr: 0.00027075
Epoch : 3, Avg Loss : 1.9200 lr: 0.00025721
Epoch : 4, Avg Loss : 1.7077 lr: 0.00024435
Epoch : 5, Avg Loss : 1.6069 lr: 0.00023213
Epoch : 6, Avg Loss : 1.4352 lr: 0.00022053
Epoch : 7, Avg Loss : 1.4784 lr: 0.00020950
Epoch : 8, Avg Loss : 1.2107 lr: 0.00019903
Epoch : 9, Avg Loss : 1.2057 lr: 0.00018907
Epoch : 10, Avg Loss : 1.2485 lr: 0.00017962
Epoch : 11, Avg Loss : 1.1269 lr: 0.00017064
Epoch : 12, Avg Loss : 1.1141 lr: 0.00016211
Epoch : 13, Avg Loss : 1.0017 lr: 0.00015400
Epoch : 14, Avg Loss : 1.0561 lr: 0.00014630
Epoch : 15, Avg Loss : 1.0362 lr: 0.00013899
Epoch : 16, Avg Loss : 1.0200 lr: 0.00013204
Epoch : 17, Avg Loss : 0.9856 lr: 0.00012544
Epoch : 18, Avg Loss : 0.9880 lr: 0.00011916
Epoch : 19, Avg Loss : 0.9686 lr: 0.00011321
Epoch : 20, Avg Loss : 0.9761 lr: 0.00010755
Epoch : 21, Avg Loss : 0.9743 lr: 0.00010217
Epoch : 22, Avg Loss : 0.8914 lr: 0.00009706
Epoch : 23, Avg Loss : 0.9043 lr: 0.00009221
Epoch : 24, Avg Loss : 0.9313 lr: 0.00008760
Epoch : 25, Avg Loss : 0.9150 lr: 0.00008322
Epoch : 26, Avg Loss : 0.8878 lr: 0.00007906
Epoch : 27, Avg Loss : 0.9015 lr: 0.00007510
Epoch : 28, Avg Loss : 0.9026 lr: 0.00007135
Epoch : 29, Avg Loss : 0.8760 lr: 0.00006778
FG 학습 완료. 이제 F의 output을 실제 dataset의 label과 연결.
Epoch : 0, Avg Loss : 2.3293 lr: 0.00030000
Epoch : 1, Avg Loss : 2.2178 lr: 0.00028500
Epoch : 2, Avg Loss : 2.1485 lr: 0.00027075
Epoch : 3, Avg Loss : 2.0859 lr: 0.00025721
Epoch : 4, Avg Loss : 2.0285 lr: 0.00024435
Epoch : 5, Avg Loss : 1.9744 lr: 0.00023213
Epoch : 6, Avg Loss : 1.9253 lr: 0.00022053
Epoch : 7, Avg Loss : 1.8813 lr: 0.00020950
Epoch : 8, Avg Loss : 1.8365 lr: 0.00019903
Epoch : 9, Avg Loss : 1.7902 lr: 0.00018907
Epoch : 10, Avg Loss : 1.7543 lr: 0.00017962
Epoch : 11, Avg Loss : 1.7324 lr: 0.00017064
Epoch : 12, Avg Loss : 1.7152 lr: 0.00016211
Epoch : 13, Avg Loss : 1.6990 lr: 0.00015400
Epoch : 14, Avg Loss : 1.6868 lr: 0.00014630
Epoch : 15, Avg Loss : 1.6766 lr: 0.00013899
Epoch : 16, Avg Loss : 1.6682 lr: 0.00013204
Epoch : 17, Avg Loss : 1.6608 lr: 0.00012544
Epoch : 18, Avg Loss : 1.6515 lr: 0.00011916
Epoch : 19, Avg Loss : 1.6468 lr: 0.00011321
Epoch : 20, Avg Loss : 1.6408 lr: 0.00010755
Epoch : 21, Avg Loss : 1.6380 lr: 0.00010217
Epoch : 22, Avg Loss : 1.6329 lr: 0.00009706
Epoch : 23, Avg Loss : 1.6268 lr: 0.00009221
Epoch : 24, Avg Loss : 1.6233 lr: 0.00008760
Epoch : 25, Avg Loss : 1.6222 lr: 0.00008322
Epoch : 26, Avg Loss : 1.6171 lr: 0.00007906
Epoch : 27, Avg Loss : 1.6141 lr: 0.00007510
Epoch : 28, Avg Loss : 1.6101 lr: 0.00007135
Epoch : 29, Avg Loss : 1.6079 lr: 0.00006778
실제 test
총 개수 : 10000
 맞춘 개수 : 4480
 정확도: 44
 찍었을 때의 정확도 : 10

종료 코드 0(으)로 완료된 프로세스
